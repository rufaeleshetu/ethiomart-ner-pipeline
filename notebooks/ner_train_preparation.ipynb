{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cab443b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"ethiomart_task2_labeled.conll\"\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"❌ File not found: {file_path}\")\n",
    "\n",
    "tokens, tags = read_conll(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa9d0696",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "file_path = os.path.join(os.getcwd(), \"ethiomart_task2_labeled.conll\")\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"❌ File not found: {file_path}\")\n",
    "\n",
    "tokens, tags = read_conll(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8abb7244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 28\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "})\n",
      "✅ Task 3 complete: Dataset ready for training\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Step 2: Helper to read CoNLL-formatted data\n",
    "def read_conll(path):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    sentence = []\n",
    "    label = []\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "                continue\n",
    "            token, tag = line.split()\n",
    "            sentence.append(token)\n",
    "            label.append(tag)\n",
    "\n",
    "    if sentence:  # add last sentence if file does not end in a blank line\n",
    "        sentences.append(sentence)\n",
    "        labels.append(label)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "# Step 3: Load labeled file\n",
    "file_path = \"ethiomart_task2_labeled.conll\"\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(f\"❌ File not found: {file_path}\")\n",
    "\n",
    "tokens, tags = read_conll(file_path)\n",
    "\n",
    "# Step 4: Create HuggingFace Dataset object\n",
    "data = {\"tokens\": tokens, \"ner_tags\": tags}\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# Step 5: Train/Test Split\n",
    "dataset_dict = DatasetDict({\n",
    "    \"train\": dataset.train_test_split(test_size=0.2, seed=42)[\"train\"],\n",
    "    \"test\": dataset.train_test_split(test_size=0.2, seed=42)[\"test\"]\n",
    "})\n",
    "\n",
    "# Step 6: Confirm structure\n",
    "print(dataset_dict)\n",
    "print(\"✅ Task 3 complete: Dataset ready for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2f655bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 28\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data = {\"tokens\": tokens, \"ner_tags\": tags}\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2, seed=42)\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "50c6b7c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Nike', 'infinity', 'flow', 'size', '4041424344', 'Price', '3500', 'birr', 'አድራሻሜክሲኮ', 'ኮሜርስ', 'ጀርባ', 'መዚድ', 'ፕላዛ', 'የመጀመሪያ', 'ደረጃ', 'እንደወጡ', '101', 'የቢሮ', 'ቁጥር', 'ያገኙናል', 'or', 'call', '0920238243', 'EthioBrandhttpstmeethio_brand_collection'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "247ff601",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(dataset_dict, DatasetDict)\n",
    "assert all(k in dataset_dict for k in [\"train\", \"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78efe8d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea35503094f41fbaa1fb690ffbc1615",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99431df424a4207972d65f41dc3aa13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict.save_to_disk(\"data/fine_tune_ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "00287780",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags = read_conll(\"ethiomart_task2_labeled.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2e2581fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 28\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "# Load the saved train/test dataset\n",
    "dataset_dict = load_from_disk(\"data/fine_tune_ready\")\n",
    "\n",
    "# Confirm structure\n",
    "print(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11773df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['Nike', 'infinity', 'flow', 'size', '4041424344', 'Price', '3500', 'birr', 'አድራሻሜክሲኮ', 'ኮሜርስ', 'ጀርባ', 'መዚድ', 'ፕላዛ', 'የመጀመሪያ', 'ደረጃ', 'እንደወጡ', '101', 'የቢሮ', 'ቁጥር', 'ያገኙናል', 'or', 'call', '0920238243', 'EthioBrandhttpstmeethio_brand_collection'], 'ner_tags': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18ac162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(set(label for sublist in dataset_dict[\"train\"][\"ner_tags\"] for label in sublist))\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8da1521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d49c56075d64f20b13fa68163528e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--Davlan--afro-xlmr-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "267ba931ffb24c2ca873e6247611c1e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc25edc1cb0d43b6b2c8a34473942303",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b18f77199cec4c5eac6c9e8dc7f89d7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7028a23fb9764fc4a46ea24faa005a59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/707 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e2338ae44a4de1b72a32dea686276a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_checkpoint = \"Davlan/afro-xlmr-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac859945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[\"train\"][0][\"ner_tags\"])\n",
    "print(type(dataset_dict[\"train\"][0][\"ner_tags\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ef20fd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label list from dataset and build label2id mapping\n",
    "label_list = list(set(tag for tags in dataset_dict[\"train\"][\"ner_tags\"] for tag in tags))\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_list))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "79ea8bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_labels(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7256ffbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768ec3a238dd4d4282e8c54089ce08a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/28 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9230817441044c338dbafe0210f11f21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_dict = dataset_dict.map(normalize_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b516415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d893f7ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_strategy=\"epoch\",\n",
    "logging_steps=10,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "966a2de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61d43443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da69e162635e41f7acb6b3eb978ac97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c289e399",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "['fine_tune_ready']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.path.exists(\"data/fine_tune_ready\"))\n",
    "print(os.listdir(\"data\") if os.path.exists(\"data\") else \"data/ folder not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e4e6fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(path):\n",
    "    sentence = []\n",
    "    label = []\n",
    "    sentences = []\n",
    "    labels = []\n",
    "\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                if sentence:\n",
    "                    sentences.append(sentence)\n",
    "                    labels.append(label)\n",
    "                    sentence = []\n",
    "                    label = []\n",
    "            else:\n",
    "                splits = line.split()\n",
    "                if len(splits) >= 2:\n",
    "                    sentence.append(splits[0])\n",
    "                    label.append(splits[-1])\n",
    "    return sentences, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "674cc5c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\notebooks\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2d18e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags = read_conll(\"ethiomart_task2_labeled.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61b00ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "data = {\"tokens\": tokens, \"ner_tags\": tags}\n",
    "dataset = Dataset.from_dict(data)\n",
    "dataset_dict = dataset.train_test_split(test_size=0.2, seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8200e73c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf359286984344b19ef3fd58fa1df9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8f4ba7623d148d590579fdca563d85a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = list(set(tag for tags in dataset_dict[\"train\"][\"ner_tags\"] for tag in tags))\n",
    "label_list.sort()\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "def normalize_labels(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset_dict = dataset_dict.map(normalize_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8c132d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(int(example[\"ner_tags\"][word_idx]))  # Cast to int\n",
    "        else:\n",
    "            labels.append(int(example[\"ner_tags\"][word_idx]))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # ✅ Debugging output inside function\n",
    "    print(\"tokens:\", example[\"tokens\"])\n",
    "    print(\"word_ids:\", word_ids)\n",
    "    print(\"aligned labels:\", labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a9b91918",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(int(example[\"ner_tags\"][word_idx]))\n",
    "        else:\n",
    "            labels.append(int(example[\"ner_tags\"][word_idx]))\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc194f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 27\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tokens', 'ner_tags'],\n",
      "        num_rows: 7\n",
      "    })\n",
      "})\n",
      "{'tokens': ['Tempered', 'Glass', 'Bakeware', 'TikTok', 'tiktokcommirtteka', 'Telegram', 'tmeMerttEka', 'ለወዳጆችዎ', 'forward', 'ያድርጉ'], 'ner_tags': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict)\n",
    "print(dataset_dict[\"train\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5348779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(set(tag for tags in dataset_dict[\"train\"][\"ner_tags\"] for tag in tags))\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_list))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "67b0ac0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c881cb7ed847a49a81a2e6d7701814",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d389b0277f64b84813c42ae327cf26c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_labels(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset_dict = dataset_dict.map(normalize_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f1ab9d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Davlan/afro-xlmr-base\")\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "323625c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(dataset_dict[\"train\"]):\n",
    "    if not isinstance(example[\"ner_tags\"], list):\n",
    "        print(f\"Row {i} is not a list:\", example[\"ner_tags\"])\n",
    "    elif any(not isinstance(tag, int) for tag in example[\"ner_tags\"]):\n",
    "        print(f\"Row {i} has non-int:\", example[\"ner_tags\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e86693c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15db46963f794fdcb7a926715f52145d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cc6e0160b2740de9c5c5073b794dc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "label_list = list(set(tag for tags in dataset_dict[\"train\"][\"ner_tags\"] for tag in tags))\n",
    "label2id = {label: idx for idx, label in enumerate(sorted(label_list))}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "def normalize_labels(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset_dict = dataset_dict.map(normalize_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "67027e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Ignore special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)  # or label[word_idx] if same word\n",
    "        labels.append(label_ids)\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b8f19284",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens', 'ner_tags']\n",
      "{'tokens': ['Tempered', 'Glass', 'Bakeware', 'TikTok', 'tiktokcommirtteka', 'Telegram', 'tmeMerttEka', 'ለወዳጆችዎ', 'forward', 'ያድርጉ'], 'ner_tags': [3, 3, 3, 3, 3, 3, 3, 3, 3, 3]}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict[\"train\"].column_names)  # Check available columns\n",
    "print(dataset_dict[\"train\"][0])           # Inspect a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2881706d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello world !'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correct format for `tokens` column:\n",
    "[\"Hello\", \"world\", \"!\"]  # ✅ List of tokens\n",
    "\"Hello world !\"           # ❌ Raw string (will fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4ebfd424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[0, 37446, 56, 297, 95384, 15082, 13, 15876, 19591, 618, 685, 2126, 6448, 277, 4326, 3135, 161, 86026, 808, 282, 30248, 3062, 647, 161, 2237, 5698, 5040, 139624, 29597, 40225, 4463, 35929, 11959, 2], [0, 2356, 205545, 8080, 3493, 15937, 34, 1212, 387, 2357, 616, 927, 83479, 62449, 313, 54003, 80588, 37082, 190, 20847, 7338, 856, 1723, 387, 2357, 4221, 11, 109561, 70, 62805, 24018, 31365, 35958, 190, 170582, 42, 43137, 1062, 427, 594, 2041, 1430, 1257, 47, 63907, 117690, 427, 31365, 136, 616, 86039, 13293, 572, 254, 22, 3033, 611, 1022, 38230, 166615, 2661, 46347, 1363, 23856, 45029, 80111, 72804, 54259, 102198, 91969, 3674, 24018, 51210, 189682, 7, 6, 28814, 33457, 1479, 71867, 297, 24018, 38730, 179519, 2037, 47, 534, 26668, 294, 94831, 2592, 23, 206, 1022, 38230, 166615, 2661, 56811, 49119, 4442, 2839, 616, 2839, 98009, 7, 19914, 113458, 2396, 16291, 208071, 106, 28941, 83010, 541, 47599, 13, 26959, 382, 8359, 104621, 758, 305, 10700, 36812, 7, 21947, 81423, 16505, 209, 21974, 2848, 81423, 6897, 26824, 106594, 7, 36495, 305, 647, 2248, 304, 1662, 14263, 116, 425, 304, 66145, 14226, 563, 16291, 43643, 44813, 39808, 254, 32863, 678, 104909, 13684, 3055, 26219, 3447, 73655, 32134, 7192, 538, 22968, 22392, 26828, 4353, 227247, 82344, 30948, 165598, 4331, 9312, 64344, 78603, 43960, 48025, 209, 87774, 263, 42, 6534, 86, 330, 2237, 179347, 204108, 2934, 4555, 12131, 5617, 714, 2947, 133249, 12975, 20543, 714, 143534, 152830, 66000, 140042, 2370, 60014, 9171, 4363, 3851, 6724, 548, 623, 6, 110538, 4363, 18420, 101954, 728, 150238, 6, 233882, 113628, 2269, 196171, 6, 70317, 5554, 3236, 29831, 11108, 2010, 14615, 5653, 2934, 204634, 33659, 116012, 29768, 816, 60204, 1178, 45254, 33805, 4463, 90850, 4799, 23380, 6, 162183, 3376, 24163, 26145, 104819, 2934, 18586, 3833, 93431, 1426, 86, 330, 164511, 277, 6, 17680, 10824, 70317, 816, 6, 17231, 946, 145984, 548, 150971, 4712, 4656, 5617, 3975, 18, 13616, 330, 164511, 2]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[-100, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100], [-100, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, -100]]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_batch = dataset_dict[\"train\"][:2]  # Test on 2 examples\n",
    "tokenize_and_align_labels(small_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6a21030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Error on row 0: 'int' object is not subscriptable\n",
      "❌ Error on row 1: 'int' object is not subscriptable\n",
      "❌ Error on row 2: 'int' object is not subscriptable\n",
      "❌ Error on row 3: 'int' object is not subscriptable\n",
      "❌ Error on row 4: 'int' object is not subscriptable\n",
      "❌ Error on row 5: 'int' object is not subscriptable\n",
      "❌ Error on row 6: 'int' object is not subscriptable\n",
      "❌ Error on row 7: 'int' object is not subscriptable\n",
      "❌ Error on row 8: 'int' object is not subscriptable\n",
      "❌ Error on row 9: 'int' object is not subscriptable\n",
      "❌ Error on row 10: 'int' object is not subscriptable\n",
      "❌ Error on row 11: 'int' object is not subscriptable\n",
      "❌ Error on row 12: 'int' object is not subscriptable\n",
      "❌ Error on row 13: 'int' object is not subscriptable\n",
      "❌ Error on row 14: 'int' object is not subscriptable\n",
      "❌ Error on row 15: 'int' object is not subscriptable\n",
      "❌ Error on row 16: 'int' object is not subscriptable\n",
      "❌ Error on row 17: 'int' object is not subscriptable\n",
      "❌ Error on row 18: 'int' object is not subscriptable\n",
      "❌ Error on row 19: 'int' object is not subscriptable\n",
      "❌ Error on row 20: 'int' object is not subscriptable\n",
      "❌ Error on row 21: 'int' object is not subscriptable\n",
      "❌ Error on row 22: 'int' object is not subscriptable\n",
      "❌ Error on row 23: 'int' object is not subscriptable\n",
      "❌ Error on row 24: 'int' object is not subscriptable\n",
      "❌ Error on row 25: 'int' object is not subscriptable\n",
      "❌ Error on row 26: 'int' object is not subscriptable\n"
     ]
    }
   ],
   "source": [
    "for i, example in enumerate(dataset_dict[\"train\"]):\n",
    "    try:\n",
    "        _ = tokenize_and_align_labels(example)\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error on row {i}:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0719b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    \n",
    "    # Initialize labels list\n",
    "    labels = []\n",
    "    \n",
    "    # Check if labels are already sequences\n",
    "    if isinstance(examples[\"ner_tags\"][0], int):\n",
    "        # Single label case - convert to list of lists\n",
    "        label_sequences = [[tag] for tag in examples[\"ner_tags\"]]\n",
    "    else:\n",
    "        label_sequences = examples[\"ner_tags\"]\n",
    "    \n",
    "    for i, label_seq in enumerate(label_sequences):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)  # Special tokens\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label_seq[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)  # Or label_seq[word_idx] if same word\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3e0b581d",
   "metadata": {},
   "outputs": [],
   "source": [
    "broken_indices = []\n",
    "for i, example in enumerate(dataset_dict[\"train\"]):\n",
    "    try:\n",
    "        _ = tokenize_and_align_labels({k: [v] for k, v in example.items()})\n",
    "    except Exception as e:\n",
    "        print(f\"Error on row {i}:\", e)\n",
    "        broken_indices.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8f4cc488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken rows found\n"
     ]
    }
   ],
   "source": [
    "# First identify all broken rows\n",
    "broken_indices = []\n",
    "for i in range(len(dataset_dict[\"train\"])):\n",
    "    try:\n",
    "        # Test if the row can be processed\n",
    "        example = dataset_dict[\"train\"][i]\n",
    "        _ = tokenize_and_align_labels({\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]})\n",
    "    except Exception as e:\n",
    "        broken_indices.append(i)\n",
    "        print(f\"Broken row {i}: {str(e)}\")\n",
    "\n",
    "# Then filter them out\n",
    "if broken_indices:\n",
    "    dataset_dict[\"train\"] = dataset_dict[\"train\"].select(\n",
    "        [i for i in range(len(dataset_dict[\"train\"])) \n",
    "        if i not in broken_indices]\n",
    "    )\n",
    "    print(f\"Removed {len(broken_indices)} broken rows\")\n",
    "else:\n",
    "    print(\"No broken rows found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb1ffc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken rows found\n"
     ]
    }
   ],
   "source": [
    "# First identify all broken rows\n",
    "broken_indices = []\n",
    "\n",
    "for i in range(len(dataset_dict[\"train\"])):\n",
    "    try:\n",
    "        example = dataset_dict[\"train\"][i]\n",
    "        # Properly format the input as a dictionary\n",
    "        _ = tokenize_and_align_labels({\"tokens\": [example[\"tokens\"]], \"ner_tags\": [example[\"ner_tags\"]]})\n",
    "    except Exception as e:\n",
    "        broken_indices.append(i)\n",
    "        print(f\"Broken row {i}: {str(e)}\")  # Fixed string formatting\n",
    "\n",
    "# Then filter them out\n",
    "if broken_indices:\n",
    "    dataset_dict[\"train\"] = dataset_dict[\"train\"].select(  # Fixed period to dot\n",
    "        [i for i in range(len(dataset_dict[\"train\"])) \n",
    "        if i not in broken_indices]\n",
    "    )\n",
    "    print(f\"Removed {len(broken_indices)} broken rows\")  # Fixed string formatting\n",
    "else:\n",
    "    print(\"No broken rows found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "edbc6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def debug_tokenize(example):\n",
    "    print(\"Input tokens:\", example[\"tokens\"])\n",
    "    print(\"Input tags:\", example[\"ner_tags\"])\n",
    "    print(\"Type of tags:\", type(example[\"ner_tags\"]))\n",
    "    # Rest of your function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5f2a8b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "models = {\n",
    "    \"xlm-roberta\": \"xlm-roberta-base\",\n",
    "    \"afroxlmr\": \"castorini/afroxlmr-large\", \n",
    "    \"mbert\": \"bert-base-multilingual-cased\",\n",
    "    \"amharic-bert\": \"Davlan/bert-base-multilingual-cased-finetuned-amharic\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "18f14fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (special tokens)\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = classification_report(true_labels, true_predictions, output_dict=True)\n",
    "    return {\n",
    "        \"precision\": results[\"weighted avg\"][\"precision\"],\n",
    "        \"recall\": results[\"weighted avg\"][\"recall\"],\n",
    "        \"f1\": results[\"weighted avg\"][\"f1-score\"],\n",
    "        \"accuracy\": results[\"accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "50423f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformers version: 4.30.2\n",
      "Datasets version: 2.12.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import datasets\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Datasets version: {datasets.__version__}\")\n",
    "# Should output versions ≥4.30.0 for full compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "51a64a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)  # Verify you're using the right Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ddd73c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.30.2\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)  # Should print version (4.30+ recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c62d48cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, example in enumerate(dataset_dict[\"train\"]):\n",
    "    if not isinstance(example, dict) or \"tokens\" not in example or \"ner_tags\" not in example:\n",
    "        print(f\"❌ Row {i} is broken:\", example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "436bf954",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_indices = [\n",
    "    i for i, example in enumerate(dataset_dict[\"train\"])\n",
    "    if isinstance(example, dict) and \"tokens\" in example and \"ner_tags\" in example\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "428c1389",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict[\"train\"] = dataset_dict[\"train\"].select(good_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cb4c9604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "285e7add15964219960937f3413e0d01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471a2ee26a5548618028060deeb54fc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5547ca2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079ea8c464514abebba7846fb13fbccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6c8b515afec4773951391c8ee02a5e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80a9d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6ce385a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86f3e846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.52.4\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b32e0ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.40.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf14421",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "66f1b718",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e394f9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tags = read_conll(\"ethiomart_task2_labeled.conll\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29f37d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "current_dir = os.path.dirname(__file__) if \"__file__\" in globals() else os.getcwd()\n",
    "file_path = os.path.join(current_dir, \"ethiomart_task2_labeled.conll\")\n",
    "tokens, tags = read_conll(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4729eb38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BABY', 'SWADDLING', 'BLANKETS', 'የጨቅላ', 'ህፃናት', 'ማቀፊያና', 'ማስተኛ', 'ብርድ', 'ልብስ', 'አዲስ', 'ለተወለዱ', 'እና', 'እስከ', '6ወር', 'ላሉ', 'ህፃናት', 'የሚሆን', 'በጣም', 'ወፍራም', 'ለስላሳ', 'ህፃናት', 'እና', 'እናቶች', 'የሚመርጡት', 'ዋጋ', '1800', 'ብር', 'በቴሌግራም', 'ለማዘዝ', 'awasadmin1', 'awasadmin2', 'አድራሻችን', '1', 'ሜክሲኮ', 'አልሳም', 'አፓርታማ', 'ግራውንድ', 'ቁ', '29', '2', 'ቦሌ', 'መድሐኔዓለም', 'ቦሌ', 'መሰናዶ', 'ትቤት', 'ፊትለፊት', 'አለምነሽ', 'ፕላዛ', 'ግራውንድ', 'ሱቅ', 'ቁጥር', '05', '251941661030', '251943190237', 'ለወዳጅዎ', 'ስላጋሩ', 'እናመሠግናለን', 'ቴሌግራም', 'ቻናል', 'tmeAwasMart']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRICE', 'O', 'B-LOCATION', 'O', 'O', 'B-PRICE', 'O', 'O', 'O', 'O', 'B-PRICE', 'O', 'O', 'O', 'O', 'O', 'O', 'B-PRICE', 'O', 'B-PRICE', 'B-PRICE', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOCATION', 'O', 'B-LOCATION', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(tokens[0])\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f230eaf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {\"tokens\": tokens, \"ner_tags\": tags}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8af2b296",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = sorted(set(tag for seq in tags for tag in seq))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5a9b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data\n",
    "train_tokens, test_tokens, train_tags, test_tags = train_test_split(tokens, tags, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DatasetDict\n",
    "train_dataset = Dataset.from_dict({\"tokens\": train_tokens, \"ner_tags\": train_tags})\n",
    "test_dataset = Dataset.from_dict({\"tokens\": test_tokens, \"ner_tags\": test_tags})\n",
    "dataset_dict = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f22fd600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf29da81fdf94c47bf224afa51390438",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e51ac08f8874cde8bdc3584a28ac0eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def normalize_labels(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset_dict = dataset_dict.map(normalize_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8450975b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], \n",
    "                                  truncation=True, \n",
    "                                  is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f45ef5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Davlan/afro-xlmr-base\"  # Or any model you're using\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "26769ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], \n",
    "                                 truncation=True, \n",
    "                                 is_split_into_words=True)\n",
    "    \n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    \n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9214b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True)\n",
    "\n",
    "    labels_batch = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        label_ids = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels_batch.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels_batch\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c2dfe8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2593c15050d94cb183b4df7abbacec10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08011eba00da4d779bcfc80ceab627bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe8ef070",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['ዉሃ',\n",
       "  'ስርገትን',\n",
       "  'ወደ',\n",
       "  'ፍራሽ',\n",
       "  'ዉስጥ',\n",
       "  'እንዳይገባ',\n",
       "  'እና',\n",
       "  'አላስፈላጊ',\n",
       "  'ሽታን',\n",
       "  'እንዲሁም',\n",
       "  'ድካምን',\n",
       "  'የሚከላከል',\n",
       "  'አንሶላ',\n",
       "  'Mattress',\n",
       "  'PROTECTOR',\n",
       "  'POLYESTER',\n",
       "  'MICROFIBER',\n",
       "  'ለ',\n",
       "  '150',\n",
       "  'ለ',\n",
       "  '180',\n",
       "  'ከነፃ',\n",
       "  'ዲሊቨሪ',\n",
       "  'ጋር',\n",
       "  '3500',\n",
       "  'ብር',\n",
       "  'አድራሻ',\n",
       "  'ቁጥር',\n",
       "  '1',\n",
       "  'ልደታ',\n",
       "  'ወደ',\n",
       "  'ባልቻ',\n",
       "  'ሆስፒታል',\n",
       "  'ገባ',\n",
       "  'ብሎ',\n",
       "  'አህመድ',\n",
       "  'ህንፃ',\n",
       "  'ላይ',\n",
       "  '1ኛፎቅ',\n",
       "  '114B',\n",
       "  'ባሉበት',\n",
       "  'ያለተጨማሪ',\n",
       "  'ክፍያ',\n",
       "  'ማዘዝ',\n",
       "  'ይችላሉ',\n",
       "  '0933334444',\n",
       "  'LeMazez_z',\n",
       "  '0946242424',\n",
       "  'Le_Mazez',\n",
       "  '0944109295',\n",
       "  'Lemaze_z'],\n",
       " 'ner_tags': [3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3],\n",
       " 'input_ids': [0,\n",
       "  6,\n",
       "  12131,\n",
       "  12611,\n",
       "  62215,\n",
       "  154659,\n",
       "  548,\n",
       "  5156,\n",
       "  29831,\n",
       "  162796,\n",
       "  186706,\n",
       "  58156,\n",
       "  21080,\n",
       "  2302,\n",
       "  57639,\n",
       "  2095,\n",
       "  210899,\n",
       "  86838,\n",
       "  145545,\n",
       "  42262,\n",
       "  60757,\n",
       "  6393,\n",
       "  43205,\n",
       "  2627,\n",
       "  177117,\n",
       "  2202,\n",
       "  20924,\n",
       "  21554,\n",
       "  4712,\n",
       "  7656,\n",
       "  87463,\n",
       "  185211,\n",
       "  441,\n",
       "  27010,\n",
       "  64353,\n",
       "  1723,\n",
       "  205885,\n",
       "  6,\n",
       "  135775,\n",
       "  14255,\n",
       "  22724,\n",
       "  24480,\n",
       "  2237,\n",
       "  3252,\n",
       "  2237,\n",
       "  8719,\n",
       "  124404,\n",
       "  29307,\n",
       "  99177,\n",
       "  12354,\n",
       "  241520,\n",
       "  7423,\n",
       "  7280,\n",
       "  97977,\n",
       "  35648,\n",
       "  140042,\n",
       "  61264,\n",
       "  106,\n",
       "  19730,\n",
       "  4555,\n",
       "  3348,\n",
       "  5156,\n",
       "  93076,\n",
       "  17231,\n",
       "  237552,\n",
       "  6,\n",
       "  21080,\n",
       "  27231,\n",
       "  115798,\n",
       "  6,\n",
       "  49334,\n",
       "  29307,\n",
       "  2269,\n",
       "  106,\n",
       "  9171,\n",
       "  31531,\n",
       "  5653,\n",
       "  44909,\n",
       "  571,\n",
       "  6,\n",
       "  66619,\n",
       "  4088,\n",
       "  16863,\n",
       "  4413,\n",
       "  16333,\n",
       "  124449,\n",
       "  206852,\n",
       "  4363,\n",
       "  9039,\n",
       "  7872,\n",
       "  94164,\n",
       "  3894,\n",
       "  66000,\n",
       "  10289,\n",
       "  88471,\n",
       "  636,\n",
       "  4613,\n",
       "  731,\n",
       "  169,\n",
       "  454,\n",
       "  169,\n",
       "  3894,\n",
       "  9271,\n",
       "  2357,\n",
       "  2357,\n",
       "  2357,\n",
       "  636,\n",
       "  454,\n",
       "  4613,\n",
       "  731,\n",
       "  169,\n",
       "  3894,\n",
       "  12465,\n",
       "  963,\n",
       "  12231,\n",
       "  8821,\n",
       "  636,\n",
       "  192,\n",
       "  731,\n",
       "  454,\n",
       "  169,\n",
       "  2],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [-100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  1,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  1,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  3,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100,\n",
       "  -100]}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b19c715a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "27fad132",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5eeb9c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b67accaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 1,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 1,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c2cbabce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True,\n",
    "                                 padding=\"max_length\")\n",
    "\n",
    "    all_labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "\n",
    "    # Force conversion to plain dict of lists (not BatchEncoding or tensors)\n",
    "    return {k: v for k, v in tokenized_inputs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9bc6403",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05ac1133480744efb727ad65d48e18eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e1458b86a054c71814bab41ca5834a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d42d751c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 1,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 1,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " 1,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " 3,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100,\n",
       " -100]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0][\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df5a2dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"],\n",
    "                                 truncation=True,\n",
    "                                 is_split_into_words=True,\n",
    "                                 padding=\"max_length\",\n",
    "                                 return_tensors=None)  # <- Do NOT return tensors here\n",
    "\n",
    "    all_labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "\n",
    "    # ⛔ Do NOT wrap with dict comprehension\n",
    "    return tokenized_inputs  # Must remain a BatchEncoding with list values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6991440e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0cb531388e7341c186f01468ff85846d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b66ac84e1ba74e929e1447d30b38f4ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4c116d05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None),\n",
       " 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
       " 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "079997af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # ensure uniform length\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7d2733de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ea8147242e46098abfaf10f7f4174b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ab496c25d0414797f966792c440e27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c687abb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_dataset[\"train\"][0][\"labels\"]) == len(tokenized_dataset[\"train\"][0][\"input_ids\"])  # should be True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b79eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    # ✅ Ensure all are int64-compatible\n",
    "    tokenized_inputs[\"input_ids\"] = [[int(x) for x in seq] for seq in tokenized_inputs[\"input_ids\"]]\n",
    "    tokenized_inputs[\"attention_mask\"] = [[int(x) for x in seq] for seq in tokenized_inputs[\"attention_mask\"]]\n",
    "    tokenized_inputs[\"labels\"] = [[int(x) for x in seq] for seq in all_labels]\n",
    "\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6acde77d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f551581651410b9a4408c5b4c4cdc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b88a90b025c443f480a7bfd6d6cc23ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aa7912c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': Sequence(feature=Value(dtype='string', id=None), length=-1, id=None), 'ner_tags': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"].features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "db70e4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    max_length = 128  # fixed value for input length\n",
    "\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    all_labels = []\n",
    "    for i, word_ids in enumerate(tokenized_inputs.word_ids(batch_index=i) for i in range(len(examples[\"tokens\"]))):\n",
    "        labels = []\n",
    "        previous_word_idx = None\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                labels.append(examples[\"ner_tags\"][i][word_idx])\n",
    "            else:\n",
    "                labels.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "\n",
    "        # ✅ Ensure label length == max_length\n",
    "        labels += [-100] * (max_length - len(labels))  # pad manually\n",
    "        labels = labels[:max_length]  # truncate if longer\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = all_labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "500d16a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef36c529573a48a18ce5a17f66d2db2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5d8515b18845278941d56f9e83e49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ef3a081c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenized_dataset[\"train\"][0][\"input_ids\"]))\n",
    "print(len(tokenized_dataset[\"train\"][0][\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6707edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = tokenized_dataset.remove_columns([\"tokens\", \"ner_tags\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "80ef85ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),\n",
       " 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),\n",
       " 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"].features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ab085a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [tensor([0, 0]), tensor([6, 6]), tensor([12131, 35772]), tensor([12611, 48064]), tensor([62215, 17733]), tensor([154659,  29597]), tensor([548,   6]), tensor([ 5156, 14414]), tensor([29831,  5653]), tensor([162796,   2269]), tensor([186706,  31867]), tensor([58156, 64019]), tensor([21080, 66890]), tensor([2302,    6]), tensor([57639, 34902]), tensor([2095, 1437]), tensor([210899,  44228]), tensor([86838, 11959]), tensor([145545, 135234]), tensor([42262,     6]), tensor([60757, 49187]), tensor([6393, 3639]), tensor([ 43205, 176354]), tensor([  2627, 131005]), tensor([177117,  79082]), tensor([2202, 1464]), tensor([20924,  6839]), tensor([21554,  5495]), tensor([4712, 4585]), tensor([  7656, 114820]), tensor([87463,   101]), tensor([185211,      6]), tensor([   441, 204198]), tensor([27010, 76094]), tensor([64353, 45196]), tensor([1723,    6]), tensor([205885,  14414]), tensor([   6, 6550]), tensor([135775,  23374]), tensor([ 14255, 122648]), tensor([22724,  1437]), tensor([24480, 76765]), tensor([ 2237, 15372]), tensor([3252, 2934]), tensor([  2237, 224235]), tensor([ 8719, 29597]), tensor([124404,  54667]), tensor([29307,     6]), tensor([99177, 14414]), tensor([12354,  5653]), tensor([241520,   2370]), tensor([ 7423, 19839]), tensor([7280,    6]), tensor([ 97977, 200583]), tensor([35648,     6]), tensor([140042,  60603]), tensor([61264, 17231]), tensor([  106, 57685]), tensor([19730,   728]), tensor([ 4555, 13357]), tensor([  3348, 113452]), tensor([ 5156, 49248]), tensor([93076, 65765]), tensor([17231,  4088]), tensor([237552,   2302]), tensor([    6, 32615]), tensor([21080,  4047]), tensor([27231, 37327]), tensor([115798,  19104]), tensor([   6, 5698]), tensor([ 49334, 116946]), tensor([29307,     6]), tensor([  2269, 196655]), tensor([106, 101]), tensor([9171, 5653]), tensor([31531,  5040]), tensor([ 5653, 25513]), tensor([44909,  1464]), tensor([ 571, 6839]), tensor([   6, 5495]), tensor([66619,  4585]), tensor([ 4088, 30535]), tensor([16863, 22538]), tensor([ 4413, 22707]), tensor([16333,  9911]), tensor([124449,   4585]), tensor([206852,   1781]), tensor([ 4363, 45196]), tensor([  9039, 122648]), tensor([7872, 1437]), tensor([94164, 33805]), tensor([ 3894, 15232]), tensor([66000, 30076]), tensor([10289, 29307]), tensor([88471, 37327]), tensor([   636, 140042]), tensor([4613, 2370]), tensor([  731, 60014]), tensor([ 169, 9171]), tensor([454, 454]), tensor([  169, 93561]), tensor([ 3894, 47227]), tensor([9271,  454]), tensor([2357, 4555]), tensor([  2357, 191973]), tensor([2357,  454]), tensor([  636, 13357]), tensor([ 454, 2202]), tensor([4613,  454]), tensor([   731, 115742]), tensor([169, 454]), tensor([ 3894, 31531]), tensor([12465,  5653]), tensor([   963, 137526]), tensor([12231, 47885]), tensor([8821,  159]), tensor([ 636, 8194]), tensor([192, 294]), tensor([ 731, 9016]), tensor([ 454, 3894]), tensor([ 169, 9550]), tensor([    2, 89709]), tensor([     1, 191539]), tensor([   1, 3894]), tensor([   1, 3882]), tensor([     1, 158332]), tensor([     1, 161025]), tensor([1, 2])], 'attention_mask': [tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([1, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1]), tensor([0, 1])], 'labels': [tensor([-100, -100]), tensor([3, 1]), tensor([-100, -100]), tensor([-100, -100]), tensor([3, 3]), tensor([-100, -100]), tensor([-100,    3]), tensor([   3, -100]), tensor([   3, -100]), tensor([-100,    3]), tensor([3, 3]), tensor([   3, -100]), tensor([-100, -100]), tensor([3, 3]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([-100,    3]), tensor([3, 1]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([   1, -100]), tensor([-100,    3]), tensor([-100,    1]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([3, 3]), tensor([-100,    3]), tensor([3, 3]), tensor([-100, -100]), tensor([-100,    3]), tensor([3, 3]), tensor([-100,    3]), tensor([-100, -100]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([-100, -100]), tensor([-100,    3]), tensor([   3, -100]), tensor([3, 3]), tensor([   3, -100]), tensor([   3, -100]), tensor([   1, -100]), tensor([-100,    3]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([-100, -100]), tensor([3, 3]), tensor([   3, -100]), tensor([1, 3]), tensor([   3, -100]), tensor([   3, -100]), tensor([   3, -100]), tensor([3, 1]), tensor([-100, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([3, 1]), tensor([-100, -100]), tensor([3, 3]), tensor([   3, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([3, 3]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([   3, -100]), tensor([3, 3]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([3, 1]), tensor([-100, -100]), tensor([   1, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([3, 1]), tensor([-100,    3]), tensor([-100, -100]), tensor([-100, -100]), tensor([3, 3]), tensor([3, 3]), tensor([-100,    3]), tensor([-100, -100]), tensor([3, 3]), tensor([3, 3]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([3, 3]), tensor([-100,    3]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([-100,    3]), tensor([-100,    3]), tensor([   3, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100,    3]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100]), tensor([-100, -100])]}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch = next(iter(DataLoader(tokenized_dataset[\"train\"], batch_size=2)))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8ff35128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': None, 'format_kwargs': {}, 'columns': ['input_ids', 'attention_mask', 'labels'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"].format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "83849216",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "273d8f1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'torch', 'format_kwargs': {}, 'columns': ['input_ids', 'attention_mask', 'labels'], 'output_all_columns': False}\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"].format)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "56af8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[     0,      6,  12131,  12611,  62215, 154659,    548,   5156,  29831,\n",
      "         162796, 186706,  58156,  21080,   2302,  57639,   2095, 210899,  86838,\n",
      "         145545,  42262,  60757,   6393,  43205,   2627, 177117,   2202,  20924,\n",
      "          21554,   4712,   7656,  87463, 185211,    441,  27010,  64353,   1723,\n",
      "         205885,      6, 135775,  14255,  22724,  24480,   2237,   3252,   2237,\n",
      "           8719, 124404,  29307,  99177,  12354, 241520,   7423,   7280,  97977,\n",
      "          35648, 140042,  61264,    106,  19730,   4555,   3348,   5156,  93076,\n",
      "          17231, 237552,      6,  21080,  27231, 115798,      6,  49334,  29307,\n",
      "           2269,    106,   9171,  31531,   5653,  44909,    571,      6,  66619,\n",
      "           4088,  16863,   4413,  16333, 124449, 206852,   4363,   9039,   7872,\n",
      "          94164,   3894,  66000,  10289,  88471,    636,   4613,    731,    169,\n",
      "            454,    169,   3894,   9271,   2357,   2357,   2357,    636,    454,\n",
      "           4613,    731,    169,   3894,  12465,    963,  12231,   8821,    636,\n",
      "            192,    731,    454,    169,      2,      1,      1,      1,      1,\n",
      "              1,      1],\n",
      "        [     0,      6,  35772,  48064,  17733,  29597,      6,  14414,   5653,\n",
      "           2269,  31867,  64019,  66890,      6,  34902,   1437,  44228,  11959,\n",
      "         135234,      6,  49187,   3639, 176354, 131005,  79082,   1464,   6839,\n",
      "           5495,   4585, 114820,    101,      6, 204198,  76094,  45196,      6,\n",
      "          14414,   6550,  23374, 122648,   1437,  76765,  15372,   2934, 224235,\n",
      "          29597,  54667,      6,  14414,   5653,   2370,  19839,      6, 200583,\n",
      "              6,  60603,  17231,  57685,    728,  13357, 113452,  49248,  65765,\n",
      "           4088,   2302,  32615,   4047,  37327,  19104,   5698, 116946,      6,\n",
      "         196655,    101,   5653,   5040,  25513,   1464,   6839,   5495,   4585,\n",
      "          30535,  22538,  22707,   9911,   4585,   1781,  45196, 122648,   1437,\n",
      "          33805,  15232,  30076,  29307,  37327, 140042,   2370,  60014,   9171,\n",
      "            454,  93561,  47227,    454,   4555, 191973,    454,  13357,   2202,\n",
      "            454, 115742,    454,  31531,   5653, 137526,  47885,    159,   8194,\n",
      "            294,   9016,   3894,   9550,  89709, 191539,   3894,   3882, 158332,\n",
      "         161025,      2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1]]), 'labels': tensor([[-100,    3, -100, -100,    3, -100, -100,    3,    3, -100,    3,    3,\n",
      "         -100,    3,    3, -100, -100,    3, -100,    3,    3, -100, -100,    1,\n",
      "         -100, -100,    3, -100, -100,    3, -100,    3, -100, -100,    3, -100,\n",
      "         -100,    3, -100, -100, -100, -100,    3,    3,    3,    3,    1, -100,\n",
      "            3, -100, -100, -100,    3,    3,    1,    3,    3,    3,    3, -100,\n",
      "         -100,    3,    3, -100,    3,    3, -100,    3,    3,    3, -100, -100,\n",
      "            3,    3, -100, -100, -100,    3, -100,    1, -100, -100,    3, -100,\n",
      "         -100, -100,    3,    3, -100, -100,    3,    3, -100, -100, -100,    3,\n",
      "         -100, -100, -100, -100, -100,    3, -100, -100, -100, -100,    3, -100,\n",
      "         -100, -100, -100,    3, -100, -100, -100, -100,    3, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [-100,    1, -100, -100,    3, -100,    3, -100, -100,    3,    3, -100,\n",
      "         -100,    3, -100, -100, -100, -100,    3,    1, -100, -100, -100, -100,\n",
      "            3,    1, -100, -100, -100,    3,    3,    3, -100,    3,    3,    3,\n",
      "         -100, -100, -100,    3, -100,    3, -100,    3, -100, -100, -100,    3,\n",
      "         -100, -100,    3, -100,    3, -100,    3, -100, -100, -100,    1, -100,\n",
      "         -100, -100,    1, -100,    3, -100, -100, -100,    3, -100, -100,    3,\n",
      "         -100,    3, -100, -100, -100,    1, -100, -100, -100,    3,    1,    3,\n",
      "         -100, -100,    3,    3,    3, -100,    3,    3, -100, -100, -100,    3,\n",
      "            3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100,    3,    3,    3, -100, -100, -100,    3,\n",
      "         -100, -100, -100,    3, -100, -100, -100, -100]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "dl = DataLoader(tokenized_dataset[\"train\"], batch_size=2)\n",
    "batch = next(iter(dl))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4cb084d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids <class 'torch.Tensor'> torch.Size([2, 128])\n",
      "attention_mask <class 'torch.Tensor'> torch.Size([2, 128])\n",
      "labels <class 'torch.Tensor'> torch.Size([2, 128])\n"
     ]
    }
   ],
   "source": [
    "for key in batch:\n",
    "    print(key, type(batch[key]), batch[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b46c8a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3442a7e4e4644911932cd78c4c00bf3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "683421eb218c424b958afef12e7ec563",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def is_valid(example):\n",
    "    return (\n",
    "        len(example[\"input_ids\"]) == 128 and\n",
    "        len(example[\"attention_mask\"]) == 128 and\n",
    "        len(example[\"labels\"]) == 128 and\n",
    "        all(isinstance(i, int) for i in example[\"labels\"])\n",
    "    )\n",
    "\n",
    "tokenized_dataset = tokenized_dataset.filter(is_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "0a2dd4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "484c9114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid(example):\n",
    "    return (\n",
    "        len(example[\"input_ids\"]) > 0 and\n",
    "        len(example[\"attention_mask\"]) == len(example[\"input_ids\"]) and\n",
    "        len(example[\"labels\"]) == len(example[\"input_ids\"]) and\n",
    "        all(isinstance(i, int) for i in example[\"labels\"])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cd71512b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 0\n"
     ]
    }
   ],
   "source": [
    "tokenized_dataset = tokenized_dataset.filter(is_valid)\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "print(\"Train:\", len(tokenized_dataset[\"train\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ee1811e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63430cc4cc944f6c99e0db1de391d126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376075cb70cd43cf9721a4dd6fa931c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "add2d666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ዉሃ', 'ስርገትን', 'ወደ', 'ፍራሽ', 'ዉስጥ', 'እንዳይገባ', 'እና', 'አላስፈላጊ', 'ሽታን', 'እንዲሁም', 'ድካምን', 'የሚከላከል', 'አንሶላ', 'Mattress', 'PROTECTOR', 'POLYESTER', 'MICROFIBER', 'ለ', '150', 'ለ', '180', 'ከነፃ', 'ዲሊቨሪ', 'ጋር', '3500', 'ብር', 'አድራሻ', 'ቁጥር', '1', 'ልደታ', 'ወደ', 'ባልቻ', 'ሆስፒታል', 'ገባ', 'ብሎ', 'አህመድ', 'ህንፃ', 'ላይ', '1ኛፎቅ', '114B', 'ባሉበት', 'ያለተጨማሪ', 'ክፍያ', 'ማዘዝ', 'ይችላሉ', '0933334444', 'LeMazez_z', '0946242424', 'Le_Mazez', '0944109295', 'Lemaze_z']\n",
      "[-100, 3, -100, -100, 3, -100, -100, 3, 3, -100, 3, 3, -100, 3, 3, -100, -100, 3, -100, 3, 3, -100, -100, 1, -100, -100, 3, -100, -100, 3, -100, 3, -100, -100, 3, -100, -100, 3, -100, -100, -100, -100, 3, 3, 3, 3, 1, -100, 3, -100, -100, -100, 3, 3, 1, 3, 3, 3, 3, -100, -100, 3, 3, -100, 3, 3, -100, 3, 3, 3, -100, -100, 3, 3, -100, -100, -100, 3, -100, 1, -100, -100, 3, -100, -100, -100, 3, 3, -100, -100, 3, 3, -100, -100, -100, 3, -100, -100, -100, -100, -100, 3, -100, -100, -100, -100, 3, -100, -100, -100, -100, 3, -100, -100, -100, -100, 3, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "51 128\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset[\"train\"][0][\"tokens\"])\n",
    "print(tokenized_dataset[\"train\"][0][\"labels\"])\n",
    "print(len(tokenized_dataset[\"train\"][0][\"tokens\"]), len(tokenized_dataset[\"train\"][0][\"labels\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "44cd743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "\n",
    "    labels = []\n",
    "    for i, word_idx in enumerate(word_ids):\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8f1a0d87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 27\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\", len(tokenized_dataset[\"train\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "665cb7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4e5d9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner-model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=5,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70bf08cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset dataset format again\n",
    "tokenized_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
    "\n",
    "# Recreate Trainer (must be AFTER set_format)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fb9ace4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1729b3288e48628059828bdcd43d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0169, 'grad_norm': 5.719838619232178, 'learning_rate': 1.761904761904762e-05, 'epoch': 0.36}\n",
      "{'loss': 0.7252, 'grad_norm': 3.863450527191162, 'learning_rate': 1.523809523809524e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da6cdd7f8c9d4f9b8a2caaee39e7f49a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.25271376967430115, 'eval_runtime': 1.6592, 'eval_samples_per_second': 4.219, 'eval_steps_per_second': 2.411, 'epoch': 1.0}\n",
      "{'loss': 0.4766, 'grad_norm': 1.8696160316467285, 'learning_rate': 1.2857142857142859e-05, 'epoch': 1.07}\n",
      "{'loss': 0.4915, 'grad_norm': 4.525208950042725, 'learning_rate': 1.0476190476190477e-05, 'epoch': 1.43}\n",
      "{'loss': 0.2619, 'grad_norm': 2.115431308746338, 'learning_rate': 8.095238095238097e-06, 'epoch': 1.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ede9413e21548d096c97280f3beffcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21657605469226837, 'eval_runtime': 1.8368, 'eval_samples_per_second': 3.811, 'eval_steps_per_second': 2.178, 'epoch': 2.0}\n",
      "{'loss': 0.4205, 'grad_norm': 3.348238945007324, 'learning_rate': 5.7142857142857145e-06, 'epoch': 2.14}\n",
      "{'loss': 0.2841, 'grad_norm': 2.4952657222747803, 'learning_rate': 3.3333333333333333e-06, 'epoch': 2.5}\n",
      "{'loss': 0.3956, 'grad_norm': 1.5027955770492554, 'learning_rate': 9.523809523809525e-07, 'epoch': 2.86}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "839db31756424ed58958a2134c866633",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.20861952006816864, 'eval_runtime': 1.7433, 'eval_samples_per_second': 4.015, 'eval_steps_per_second': 2.294, 'epoch': 3.0}\n",
      "{'train_runtime': 194.4023, 'train_samples_per_second': 0.417, 'train_steps_per_second': 0.216, 'train_loss': 0.49043726992039455, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42, training_loss=0.49043726992039455, metrics={'train_runtime': 194.4023, 'train_samples_per_second': 0.417, 'train_steps_per_second': 0.216, 'total_flos': 5291354999808.0, 'train_loss': 0.49043726992039455, 'epoch': 3.0})"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "85946ec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./ner-model\\\\tokenizer_config.json',\n",
       " './ner-model\\\\special_tokens_map.json',\n",
       " './ner-model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./ner-model\")\n",
    "tokenizer.save_pretrained(\"./ner-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "566b4951",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load your fine-tuned model and tokenizer\n",
    "ner_pipeline = pipeline(\"ner\", model=\"./ner-model\", tokenizer=\"./ner-model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Example Amharic text\n",
    "text = \"አዲስ አበባ ውስጥ ከተማዋ ላይ በ300 ብር ቤት ሽያጭ ተካሄዷል።\"\n",
    "\n",
    "# Run prediction\n",
    "results = ner_pipeline(text)\n",
    "for entity in results:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "63039f63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8e91af3d28d438daacc8678cdf3d2ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.20861952006816864,\n",
       " 'eval_runtime': 2.8394,\n",
       " 'eval_samples_per_second': 2.465,\n",
       " 'eval_steps_per_second': 1.409,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2e1aa265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./final_ner_model\\\\tokenizer_config.json',\n",
       " './final_ner_model\\\\special_tokens_map.json',\n",
       " './final_ner_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./final_ner_model\")\n",
    "tokenizer.save_pretrained(\"./final_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc4e5c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "842a3c1135834b58b260a84697c4749d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()  # Paste your token with write access\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d28a8453",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0500c05e205041a1bd962e9e9343b957",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e62b6944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d595aeea1be44b294065fcb89fcac6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95bf8f143adb4f6888b6c8f00b6904ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--rufeshe--ethio-ner-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a190d631c0945b8a22f40dd698a088f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rufeshe/ethio-ner-model/commit/79ddc3d8aa27d69851a529868c9917cc33ed6131', commit_message='Upload tokenizer', commit_description='', oid='79ddc3d8aa27d69851a529868c9917cc33ed6131', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rufeshe/ethio-ner-model', endpoint='https://huggingface.co', repo_type='model', repo_id='rufeshe/ethio-ner-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model.push_to_hub(\"rufeshe/ethio-ner-model\")\n",
    "tokenizer.push_to_hub(\"rufeshe/ethio-ner-model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bba0da52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05036b391d6c4b7a9e5c646bd39c82a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  44%|####3     | 870M/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/rufeshe/ethio-ner-model/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0395f2343654ebfb3f16cc2091e82b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  44%|####3     | 870M/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/rufeshe/ethio-ner-model/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4b29422b0674f4190a41030cdb2a9cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  48%|####7     | 1.02G/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/rufeshe/ethio-ner-model/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a89d58961684ff694848052c43b6e93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  48%|####7     | 1.02G/2.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--rufeshe--ethio-ner-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ec5977546444d3afb3b5810313724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f18b0e3d17db4ae09e2664b3378e8d46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8cfdb1c8a2e4ae18d486e264ab1de7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    tokenizer=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "result = ner_pipeline(\"እትቦች በኢትዮጵያ ናቸው\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ea12e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    tokenizer=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "result = ner_pipeline(\"እኔም በአማራ ቋንቋ እማራለሁ\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224e502f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    \"ዶክተር መኩሳ በአዲስ አበባ የሚኖር ሐኪም ነው።\",\n",
    "    \"ኢትዮጵያ በአፍሪካ ምሥራቅ አገር ነች።\",\n",
    "    \"ሩፌሴ በሶፍትዌር ኢንጅነሪንግ ላይ ሥራ እየሠራ ነው።\"\n",
    "]\n",
    "for text in texts:\n",
    "    print(ner_pipeline(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92eec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd4bb298d37c4aceac81bb20b11ae490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict, load_dataset\n",
    "\n",
    "dataset_dict = load_dataset(\"text\", data_files={\"train\": \"ethiomart_task2_labeled.conll\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88f75e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    # add logic for aligning ner_tags here\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b80d925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"text\", data_files=\"ethiomart_task2_labeled.conll\")  # adjust to your filename\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d345f9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dict = DatasetDict({\"train\": dataset[\"train\"]})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b2d204a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1891\n",
      "    })\n",
      "})\n",
      "{'text': 'BABY O'}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dict)\n",
    "print(dataset_dict[\"train\"][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd3f2a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    return tokenizer(example[\"text\"], truncation=True, padding='max_length')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "810682a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f93cdda3abde42d2a0bbe1db480656d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1891 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset_dict.map(tokenize_and_align_labels, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c54be15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0e639b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"Davlan/xlm-roberta-base-ner-hrl\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "feb0deaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:943: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a71bcfe35b7844c79d1d2326fc54db0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/953 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--rufeshe--ethio-ner-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d492c7a2b34f5e91f4b31585441753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "530d7c1fe2f84e36ae8b5132e7c7e6a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5074c8ec7fb943ff94960d58bbf8ff2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00714701cf454825a28732e1c103b9ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"አዋስማርት በአዲስ ሲስተም ላይ አዳዲስ ሞዴሎችን አመጣ።\"\n",
    "print(ner_pipeline(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0cdf8e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: gradio\n"
     ]
    }
   ],
   "source": [
    "pip show gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "363c210f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "def predict(text):\n",
    "    return ner_pipeline(text)\n",
    "\n",
    "gr.Interface(fn=predict, inputs=\"text\", outputs=\"json\", title=\"Amharic NER\").launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec54f22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8527c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load the model from Hugging Face\n",
    "ner_pipeline = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Test text\n",
    "text = \"አቶ ሙሴታች ማኅበረሰብ በአዲስ አበባ ተቀመጠ።\"\n",
    "\n",
    "# Run prediction\n",
    "output = ner_pipeline(text)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10591363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded ✅\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "print(\"Model and tokenizer loaded ✅\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d567cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: {0: 'B-LOCATION', 1: 'B-PRICE', 2: 'B-PRODUCT', 3: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name)\n",
    "print(\"Labels:\", config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49440b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁እኔ', '▁', 'መተ', 'ከል', '▁ምርምር', '▁ማቅረብ', 'ና', '▁በኢትዮጵያ', '▁እንደ', '▁ተቋ', 'ማችን']\n"
     ]
    }
   ],
   "source": [
    "text = \"እኔ መተከል ምርምር ማቅረብና በኢትዮጵያ እንደ ተቋማችን\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "333a6ecc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Inference text\n",
    "text = \"እኔ ማንነቴ የእኔን ስምና የቤተሰብ ሁኔታ ተሸክመኝ\"\n",
    "\n",
    "# Run prediction\n",
    "result = ner(text)\n",
    "\n",
    "# Display result\n",
    "from pprint import pprint\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e40783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁እኔ', '▁ማን', 'ነ', 'ቴ', '▁የ', 'እኔ', 'ን', '▁የቤተ', 'ሰብ', '▁ሁኔታ', '▁ተሸ', 'ክ', 'መ', 'ኝ']\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "tokens = tokenizer.tokenize(\"እኔ ማንነቴ የእኔን የቤተሰብ ሁኔታ ተሸክመኝ\")\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8488f1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"አቶ ሙሉጌታ በአዲስ አበባ ይኖራሉ።\"  # Example: Contains a person and location\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a353e1a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOCATION', 1: 'B-PRICE', 2: 'B-PRODUCT', 3: 'O'}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50249c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregation_strategy=\"simple\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4915d78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Create NER pipeline\n",
    "ner_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfbb1c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Sample input (Amharic)\n",
    "text = \"እሱ ማንነቱ ሲነጠብ ስለ ድርጅቱ ብቻ ተናገረ\"\n",
    "\n",
    "# Run prediction\n",
    "result = ner_pipeline(text)\n",
    "\n",
    "# Print output\n",
    "from pprint import pprint\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a57f7efe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>: O\n",
      "▁እን: O\n",
      "ቁ: O\n",
      "ላል: O\n",
      "▁በነ: O\n",
      "ዳ: O\n",
      "ጅ: O\n",
      "▁ላይ: O\n",
      "▁የተ: O\n",
      "ጠ: O\n",
      "በ: O\n",
      "ሰ: O\n",
      "▁እና: O\n",
      "▁ቤተ: O\n",
      "▁ሰ: O\n",
      "ቡን: O\n",
      "</s>: O\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "text = \"እንቁላል በነዳጅ ላይ የተጠበሰ እና ቤተ ሰቡን\"\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "\n",
    "# Get logits (raw predictions)\n",
    "logits = outputs.logits\n",
    "\n",
    "# Get predicted class index\n",
    "predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "# Map class index to label\n",
    "labels = [model.config.id2label[p.item()] for p in predictions[0]]\n",
    "\n",
    "# Map tokens to labels\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "for token, label in zip(tokens, labels):\n",
    "    print(f\"{token}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c3093d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "ner_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae7ad0b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOCATION', 1: 'B-PRICE', 2: 'B-PRODUCT', 3: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "628708c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = \"ቤቱን በ500,000 ብር ለነዋሪ ተሸጠ።\"\n",
    "result = ner_pipeline(text)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7541c443",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"የእህል እቃ በ500 ብር ተሸጠ።\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2100bdf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁የእ', 'ህል', '▁እ', 'ቃ', '▁በ', '500', '▁ብር', '▁ተሸ', 'ጠ', '።']\n",
      "[30179, 134045, 4708, 6550, 728, 4283, 35648, 178104, 6839, 2532]\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(text)\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(tokens)\n",
    "print(ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40d50f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "import torch\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "print(predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "00c69dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "predicted_ids = predictions[0].tolist()\n",
    "decoded_labels = [model.config.id2label[idx] for idx in predicted_ids]\n",
    "print(decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45ed4fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"አባ ገብረ መስቀል በ200 ብር ተገዝቷል።\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5870bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"ቤት ዋጋ በ500 ብር ነው\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7ef0c481",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"አዲስ አበባ ውስጥ መኪናው በ200 ብር ሽያጭ ነው\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7de8e5fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID 0: B-LOCATION\n",
      "ID 1: B-PRICE\n",
      "ID 2: B-PRODUCT\n",
      "ID 3: O\n"
     ]
    }
   ],
   "source": [
    "for i, label in model.config.id2label.items():\n",
    "    print(f\"ID {i}: {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c604c075",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('▁አዲስ', 'O'), ('▁አበባ', 'O'), ('▁ውስጥ', 'O'), ('▁አዲስ', 'O'), ('▁', 'O'), ('መኪና', 'O'), ('▁በ', 'O'), ('500', 'O'), ('▁ብር', 'O'), ('▁ተሸ', 'O'), ('ጠ', 'O'), ('።', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = \"አዲስ አበባ ውስጥ አዲስ መኪና በ500 ብር ተሸጠ።\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "predicted_ids = predictions[0].tolist()\n",
    "decoded_labels = [model.config.id2label[idx] for idx in predicted_ids]\n",
    "\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(list(zip(tokens, decoded_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5fac686c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"አዲስ አበባ ውስጥ መኪና በ500 ብር ሽያጭ ላይ ነው\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfd28649",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"rufeshe/ethio-ner-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a9c073dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--rufeshe--ethio-ner-model. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/rufeshe/ethio-ner-model/commit/f008ee81c25b6bbcce29a71d2b1819798f9961ac', commit_message='Upload tokenizer', commit_description='', oid='f008ee81c25b6bbcce29a71d2b1819798f9961ac', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rufeshe/ethio-ner-model', endpoint='https://huggingface.co', repo_type='model', repo_id='rufeshe/ethio-ner-model'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"rufeshe/ethio-ner-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "071379aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Error while downloading from https://huggingface.co/rufeshe/ethio-ner-model/resolve/main/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"እኔ ማንኛውንም ምርት በ500 ብር አገኘሁ።\"\n",
    "result = ner(text)\n",
    "from pprint import pprint\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20be0564",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"rufeshe/ethio-ner-model\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "text = \"እንቅስቃሴውን ምርት እና በ500 ብር ተሸጠ።\"\n",
    "result = ner(text)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c37f28b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = [id2label[idx] for idx in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c3a7518",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = [id2label.get(idx, id2label.get(str(idx), \"UNKNOWN\")) for idx in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd18ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "ner_pipeline = pipeline(\"token-classification\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6426256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>             -> UNKNOWN\n",
      "▁እኔ             -> UNKNOWN\n",
      "▁እ              -> UNKNOWN\n",
      "ባብ              -> UNKNOWN\n",
      "▁መ              -> UNKNOWN\n",
      "ሸ               -> UNKNOWN\n",
      "ጫ               -> UNKNOWN\n",
      "▁ቤት             -> UNKNOWN\n",
      "▁በ              -> UNKNOWN\n",
      "500             -> UNKNOWN\n",
      "▁ብር             -> UNKNOWN\n",
      "▁አለኝ            -> UNKNOWN\n",
      "።               -> UNKNOWN\n",
      "</s>            -> UNKNOWN\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "text = \"እኔ እባብ መሸጫ ቤት በ500 ብር አለኝ።\"\n",
    "\n",
    "# Load model\n",
    "model_name = \"rufeshe/ethio-ner-model\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n",
    "\n",
    "# Get logits and predictions\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "predictions = torch.argmax(logits, dim=-1)[0].tolist()\n",
    "\n",
    "# Decode predictions\n",
    "id2label = model.config.id2label\n",
    "decoded = [id2label.get(str(idx), \"UNKNOWN\") for idx in predictions]\n",
    "\n",
    "# Print results\n",
    "for token, label in zip(tokens, decoded):\n",
    "    print(f\"{token:<15} -> {label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17106c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "14\n",
      "<class 'int'>\n"
     ]
    }
   ],
   "source": [
    "print(type(predictions))\n",
    "print(len(predictions))\n",
    "print(type(predictions[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9f008c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions is a list of integers (label IDs)\n",
    "predicted_ids = predictions  # Already your predictions\n",
    "\n",
    "# Convert IDs to label names\n",
    "decoded_labels = [\n",
    "    model.config.id2label.get(str(idx), \"UNKNOWN\") for idx in predicted_ids\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c4d44c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted IDs: [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n",
      "Decoded Labels: ['UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN', 'UNKNOWN']\n"
     ]
    }
   ],
   "source": [
    "print(\"Predicted IDs:\", predictions)\n",
    "print(\"Decoded Labels:\", decoded_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "35bd938b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_labels = [\n",
    "    model.config.id2label.get(idx, \"UNKNOWN\") for idx in predicted_ids\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c41abc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'B-LOCATION', 1: 'B-PRICE', 2: 'B-PRODUCT', 3: 'O'}\n"
     ]
    }
   ],
   "source": [
    "print(model.config.id2label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ecff891c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No evaluation results found.\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Step 1: Check if evaluation metrics exist\n",
    "try:\n",
    "    assert 'model_metrics' in locals() or 'model_metrics' in globals(), \"No evaluation results found.\"\n",
    "\n",
    "    if isinstance(model_metrics, list):\n",
    "        print(\"✅ Found multiple model evaluations.\")\n",
    "        for entry in model_metrics:\n",
    "            print(f\"Model: {entry.get('Model', 'N/A')}, F1: {entry.get('F1', 'N/A')}, Precision: {entry.get('Precision', 'N/A')}, Recall: {entry.get('Recall', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"⚠️ `model_metrics` exists but is not a list of models. Expected a structure like:\")\n",
    "        print(\"[{'Model': 'XLM-R', 'F1': ..., 'Precision': ..., 'Recall': ...}, ...]\")\n",
    "except AssertionError as e:\n",
    "    print(\"❌\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f66a2970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ No evaluated model variables found (e.g., no xlm_model_results or bert_eval).\n"
     ]
    }
   ],
   "source": [
    "# 🔍 Check if you have saved results for known models\n",
    "model_names = [\"xlm\", \"bert\", \"afro\", \"distil\"]  # common substrings for Task 4\n",
    "evaluated_models = []\n",
    "\n",
    "for var in list(globals().keys()):\n",
    "    for name in model_names:\n",
    "        if name.lower() in var.lower():\n",
    "            evaluated_models.append(var)\n",
    "\n",
    "if evaluated_models:\n",
    "    print(\"✅ You seem to have evaluated the following models:\")\n",
    "    for m in set(evaluated_models):\n",
    "        print(\"-\", m)\n",
    "else:\n",
    "    print(\"❌ No evaluated model variables found (e.g., no xlm_model_results or bert_eval).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bc8c3e5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.7.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from scikit-learn) (2.3.1)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Using cached scikit_learn-1.7.0-cp311-cp311-win_amd64.whl (10.7 MB)\n",
      "Using cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.16.0-cp311-cp311-win_amd64.whl (38.6 MB)\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/38.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/38.6 MB 1.2 MB/s eta 0:00:32\n",
      "    --------------------------------------- 0.8/38.6 MB 958.5 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.0/38.6 MB 1.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 1.0/38.6 MB 1.1 MB/s eta 0:00:36\n",
      "   - -------------------------------------- 1.3/38.6 MB 932.1 kB/s eta 0:00:40\n",
      "   - -------------------------------------- 1.6/38.6 MB 1.0 MB/s eta 0:00:37\n",
      "   - -------------------------------------- 1.8/38.6 MB 996.7 kB/s eta 0:00:37\n",
      "   - -------------------------------------- 1.8/38.6 MB 996.7 kB/s eta 0:00:37\n",
      "   -- ------------------------------------- 2.1/38.6 MB 883.1 kB/s eta 0:00:42\n",
      "   -- ------------------------------------- 2.1/38.6 MB 883.1 kB/s eta 0:00:42\n",
      "   -- ------------------------------------- 2.6/38.6 MB 926.3 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 2.6/38.6 MB 926.3 kB/s eta 0:00:39\n",
      "   -- ------------------------------------- 2.9/38.6 MB 911.8 kB/s eta 0:00:40\n",
      "   --- ------------------------------------ 3.1/38.6 MB 900.3 kB/s eta 0:00:40\n",
      "   --- ------------------------------------ 3.4/38.6 MB 923.6 kB/s eta 0:00:39\n",
      "   --- ------------------------------------ 3.4/38.6 MB 923.6 kB/s eta 0:00:39\n",
      "   --- ------------------------------------ 3.4/38.6 MB 923.6 kB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 3.9/38.6 MB 903.3 kB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 4.5/38.6 MB 972.5 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 4.5/38.6 MB 972.5 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 4.5/38.6 MB 972.5 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 4.5/38.6 MB 972.5 kB/s eta 0:00:36\n",
      "   ---- ----------------------------------- 4.7/38.6 MB 874.8 kB/s eta 0:00:39\n",
      "   ---- ----------------------------------- 4.7/38.6 MB 874.8 kB/s eta 0:00:39\n",
      "   ----- ---------------------------------- 5.0/38.6 MB 850.7 kB/s eta 0:00:40\n",
      "   ----- ---------------------------------- 5.2/38.6 MB 878.1 kB/s eta 0:00:38\n",
      "   ----- ---------------------------------- 5.5/38.6 MB 899.6 kB/s eta 0:00:37\n",
      "   ----- ---------------------------------- 5.8/38.6 MB 898.8 kB/s eta 0:00:37\n",
      "   ------ --------------------------------- 6.3/38.6 MB 941.2 kB/s eta 0:00:35\n",
      "   ------ --------------------------------- 6.6/38.6 MB 945.2 kB/s eta 0:00:34\n",
      "   ------- -------------------------------- 6.8/38.6 MB 957.6 kB/s eta 0:00:34\n",
      "   ------- -------------------------------- 7.1/38.6 MB 978.0 kB/s eta 0:00:33\n",
      "   ------- -------------------------------- 7.3/38.6 MB 980.5 kB/s eta 0:00:32\n",
      "   -------- ------------------------------- 7.9/38.6 MB 1.0 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 7.9/38.6 MB 1.0 MB/s eta 0:00:31\n",
      "   -------- ------------------------------- 8.7/38.6 MB 1.1 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 8.9/38.6 MB 1.1 MB/s eta 0:00:29\n",
      "   --------- ------------------------------ 9.2/38.6 MB 1.1 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 9.2/38.6 MB 1.1 MB/s eta 0:00:28\n",
      "   --------- ------------------------------ 9.4/38.6 MB 1.0 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 9.7/38.6 MB 1.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 10.0/38.6 MB 1.1 MB/s eta 0:00:28\n",
      "   ---------- ----------------------------- 10.5/38.6 MB 1.1 MB/s eta 0:00:27\n",
      "   ----------- ---------------------------- 10.7/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 11.0/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 11.3/38.6 MB 1.1 MB/s eta 0:00:26\n",
      "   ----------- ---------------------------- 11.5/38.6 MB 1.1 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 11.8/38.6 MB 1.1 MB/s eta 0:00:25\n",
      "   ------------ --------------------------- 12.1/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------ --------------------------- 12.3/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 12.6/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 12.8/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 12.8/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 12.8/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 13.1/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   ------------- -------------------------- 13.4/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 13.6/38.6 MB 1.1 MB/s eta 0:00:24\n",
      "   -------------- ------------------------- 13.9/38.6 MB 1.1 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 14.2/38.6 MB 1.1 MB/s eta 0:00:23\n",
      "   -------------- ------------------------- 14.4/38.6 MB 1.1 MB/s eta 0:00:23\n",
      "   --------------- ------------------------ 14.9/38.6 MB 1.1 MB/s eta 0:00:22\n",
      "   ---------------- ----------------------- 15.5/38.6 MB 1.1 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 15.7/38.6 MB 1.1 MB/s eta 0:00:21\n",
      "   ---------------- ----------------------- 16.3/38.6 MB 1.1 MB/s eta 0:00:20\n",
      "   ---------------- ----------------------- 16.3/38.6 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 16.5/38.6 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 16.8/38.6 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------------- ---------------------- 17.0/38.6 MB 1.1 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 17.6/38.6 MB 1.1 MB/s eta 0:00:19\n",
      "   ------------------ --------------------- 18.1/38.6 MB 1.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 18.4/38.6 MB 1.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 18.6/38.6 MB 1.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 18.6/38.6 MB 1.2 MB/s eta 0:00:18\n",
      "   ------------------- -------------------- 19.1/38.6 MB 1.2 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 19.4/38.6 MB 1.2 MB/s eta 0:00:17\n",
      "   -------------------- ------------------- 19.9/38.6 MB 1.2 MB/s eta 0:00:16\n",
      "   -------------------- ------------------- 20.2/38.6 MB 1.2 MB/s eta 0:00:16\n",
      "   --------------------- ------------------ 20.7/38.6 MB 1.2 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 21.2/38.6 MB 1.2 MB/s eta 0:00:15\n",
      "   ---------------------- ----------------- 21.8/38.6 MB 1.2 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 22.3/38.6 MB 1.2 MB/s eta 0:00:14\n",
      "   ----------------------- ---------------- 22.8/38.6 MB 1.3 MB/s eta 0:00:13\n",
      "   ------------------------ --------------- 23.3/38.6 MB 1.3 MB/s eta 0:00:12\n",
      "   ------------------------ --------------- 23.6/38.6 MB 1.3 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 24.1/38.6 MB 1.3 MB/s eta 0:00:12\n",
      "   ------------------------- -------------- 24.6/38.6 MB 1.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 25.2/38.6 MB 1.3 MB/s eta 0:00:11\n",
      "   -------------------------- ------------- 25.7/38.6 MB 1.3 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 26.0/38.6 MB 1.3 MB/s eta 0:00:10\n",
      "   --------------------------- ------------ 26.5/38.6 MB 1.3 MB/s eta 0:00:10\n",
      "   ---------------------------- ----------- 27.0/38.6 MB 1.3 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 27.5/38.6 MB 1.4 MB/s eta 0:00:09\n",
      "   ----------------------------- ---------- 28.0/38.6 MB 1.4 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 28.6/38.6 MB 1.4 MB/s eta 0:00:08\n",
      "   ------------------------------ --------- 29.1/38.6 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 29.4/38.6 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 29.6/38.6 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 29.9/38.6 MB 1.4 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 30.4/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.7/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 30.7/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 30.9/38.6 MB 1.4 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 31.5/38.6 MB 1.3 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 31.7/38.6 MB 1.3 MB/s eta 0:00:06\n",
      "   --------------------------------- ------ 32.0/38.6 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.0/38.6 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.2/38.6 MB 1.3 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 32.2/38.6 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 33.0/38.6 MB 1.3 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 33.6/38.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 34.1/38.6 MB 1.3 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 34.6/38.6 MB 1.3 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 35.1/38.6 MB 1.4 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 35.9/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 35.9/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 36.2/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 36.7/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.0/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.0/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 37.0/38.6 MB 1.4 MB/s eta 0:00:02\n",
      "   ---------------------------------------  37.7/38.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.3/38.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  38.5/38.6 MB 1.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 38.6/38.6 MB 1.3 MB/s eta 0:00:00\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.16.0 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "15692639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your label mappings\n",
    "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]  # adjust if you added DELIVERY_FEE, etc.\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "NUM_LABELS = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "808651ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (2.3.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (2.3.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (0.33.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohttp-3.12.13-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.7.0-cp311-cp311-win_amd64.whl.metadata (19 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.5.0-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.2-cp311-cp311-win_amd64.whl.metadata (12 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.1-cp311-cp311-win_amd64.whl.metadata (76 kB)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.14.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.6.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Using cached aiohttp-3.12.13-cp311-cp311-win_amd64.whl (451 kB)\n",
      "Using cached multidict-6.5.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached yarl-1.20.1-cp311-cp311-win_amd64.whl (86 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.7.0-cp311-cp311-win_amd64.whl (44 kB)\n",
      "Using cached propcache-0.3.2-cp311-cp311-win_amd64.whl (41 kB)\n",
      "Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   --- ------------------------------------  1/13 [pyarrow]\n",
      "   ------ ---------------------------------  2/13 [propcache]\n",
      "   --------- ------------------------------  3/13 [multidict]\n",
      "  Attempting uninstall: fsspec\n",
      "   --------- ------------------------------  3/13 [multidict]\n",
      "    Found existing installation: fsspec 2025.5.1\n",
      "   --------- ------------------------------  3/13 [multidict]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "    Uninstalling fsspec-2025.5.1:\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "      Successfully uninstalled fsspec-2025.5.1\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   ------------ ---------------------------  4/13 [fsspec]\n",
      "   --------------- ------------------------  5/13 [frozenlist]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   ------------------ ---------------------  6/13 [dill]\n",
      "   --------------------- ------------------  7/13 [aiohappyeyeballs]\n",
      "   ------------------------ ---------------  8/13 [yarl]\n",
      "   ------------------------ ---------------  8/13 [yarl]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------- ------------  9/13 [multiprocess]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   --------------------------------- ------ 11/13 [aiohttp]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ------------------------------------ --- 12/13 [datasets]\n",
      "   ---------------------------------------- 13/13 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.13 aiosignal-1.3.2 datasets-3.6.0 dill-0.3.8 frozenlist-1.7.0 fsspec-2025.3.0 multidict-6.5.0 multiprocess-0.70.16 propcache-0.3.2 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.1\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e87049b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "babf05d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Validation dataset loaded with 7 samples\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "val_dataset = load_from_disk(\"data/fine_tune_ready/test\")\n",
    "\n",
    "print(\"✅ Validation dataset loaded with\", len(val_dataset), \"samples\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f9500cde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Using cached accelerate-1.8.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (6.0.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (2.7.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (0.33.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from accelerate) (0.5.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2025.3.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub>=0.21.0->accelerate) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2025.6.15)\n",
      "Using cached accelerate-1.8.1-py3-none-any.whl (365 kB)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c020e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (2.7.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b31218d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb5cf22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "NUM_LABELS = len(label_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8f19480",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForTokenClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"xlm-roberta-base\",  # or \"final_ner_model\" if local fine-tuned\n",
    "    num_labels=NUM_LABELS,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dfbf4e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tokens', 'ner_tags']\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset.column_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29134846",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 125.11 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# 1. Label map\n",
    "label_list = [\"O\", \"B-Product\", \"I-Product\", \"B-LOC\", \"I-LOC\", \"B-PRICE\", \"I-PRICE\"]\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "# 2. Convert ner_tags from string to integer IDs\n",
    "def convert_tags_to_ids(example):\n",
    "    example[\"ner_tags\"] = [label2id[tag] for tag in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "val_dataset = val_dataset.map(convert_tags_to_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17b46cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 77.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0054a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Convert ner_tags to int IDs (you did this ✅)\n",
    "# 2. Tokenize and align (you did this ✅)\n",
    "# 3. Set format for PyTorch (this is missing):\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32a54c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0dbff941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",        # 💡 FIX: pad to consistent length\n",
    "        max_length=128,              # or pick based on your training config\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\"          # 💡 ensure outputs are torch-ready\n",
    "    )\n",
    "    \n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "    labels = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c70a76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 59.02 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19bdb62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",   # ✅ ensures uniform length\n",
    "        max_length=128,         # ✅ ensure all samples have 128 tokens\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "81100bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 68.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "03b5f8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 62.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(example[\"tokens\"],\n",
    "                          padding=True,\n",
    "                          truncation=True,\n",
    "                          is_split_into_words=True)\n",
    "    \n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "    \n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n",
    "\n",
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "10f5179f",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.set_format(\n",
    "    type=\"torch\", \n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "25ff8267",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        padding=\"max_length\",         # ✅ Fix: pad to max_length\n",
    "        truncation=True,              # ✅ Fix: truncate too-long sequences\n",
    "        is_split_into_words=True,\n",
    "        return_tensors=\"pt\"           # Optional: ensures tensors are returned\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids()\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d103ed6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 29.10 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_and_align_labels, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d2d46e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset.set_format(\n",
    "    type=\"torch\",\n",
    "    columns=[\"input_ids\", \"attention_mask\", \"labels\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a5f3b4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--xlm-roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e3aabefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Ensures all tensors are same length\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids(batch_index=0)\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2ffe890d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 40.53 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f40e1211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"xlm-roberta-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "432c91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",      # ✅ this ensures tensors are equal length\n",
    "        truncation=True,           # ✅ prevents overflow\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"        # ✅ ensures PyTorch format\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids(batch_index=0)  # needed to align tags\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return {k: v.squeeze() if isinstance(v, torch.Tensor) else v for k, v in tokenized.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6bc4247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(example):\n",
    "    tokenized = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized.word_ids(batch_index=0)\n",
    "    labels = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        else:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "\n",
    "    tokenized[\"labels\"] = labels\n",
    "\n",
    "    return {k: v.squeeze() if hasattr(v, \"squeeze\") else v for k, v in tokenized.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7b2bb6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7/7 [00:00<00:00, 63.63 examples/s]\n"
     ]
    }
   ],
   "source": [
    "val_dataset = val_dataset.map(tokenize_and_align_labels)\n",
    "val_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4fd8f739",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_24920\\3597968349.py:10: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 : < :]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Eval Result: {'eval_loss': 1.654377818107605, 'eval_model_preparation_time': 0.012, 'eval_precision': 0.8626647372215991, 'eval_recall': 0.9287974683544303, 'eval_f1': 0.8945104412207557, 'eval_runtime': 3.7731, 'eval_samples_per_second': 1.855, 'eval_steps_per_second': 0.265}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_eval_batch_size=8,\n",
    "    do_eval=True,\n",
    "    logging_strategy=\"no\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,  # optional, if defined\n",
    "    args=args\n",
    ")\n",
    "\n",
    "eval_result = trainer.evaluate()\n",
    "print(\"✅ Eval Result:\", eval_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aa502d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels.flatten(), predictions.flatten(), average='macro')\n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ac2d53bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('fine_tuned_model\\\\tokenizer_config.json',\n",
       " 'fine_tuned_model\\\\special_tokens_map.json',\n",
       " 'fine_tuned_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"fine_tuned_model\")\n",
    "tokenizer.save_pretrained(\"fine_tuned_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e28ac542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ethio_ner_model\\\\tokenizer_config.json',\n",
       " 'ethio_ner_model\\\\special_tokens_map.json',\n",
       " 'ethio_ner_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "\n",
    "model.save_pretrained(\"ethio_ner_model\")\n",
    "tokenizer.save_pretrained(\"ethio_ner_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62acbd33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📌 Final Vendor Summary\n",
      "- Posting Frequency: 28.00 posts/week\n",
      "- Average Views per Post: 0.00\n",
      "- Average Price Point: nan ETB\n",
      "- Lending Score: 14.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from transformers import pipeline\n",
    "\n",
    "# Step 1: Load cleaned vendor data\n",
    "path = os.path.join(\"..\", \"data\", \"cleaned\", \"AwasMart_cleaned.csv\")\n",
    "df = pd.read_csv(path)\n",
    "\n",
    "# Step 2: Prepare timestamp column\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Step 3: Calculate posting frequency\n",
    "weeks_active = (df['timestamp'].max() - df['timestamp'].min()).days / 7\n",
    "posting_freq = len(df) / weeks_active\n",
    "\n",
    "# Step 4: Load NER model from Hugging Face\n",
    "ner_pipe = pipeline(\"token-classification\", model=\"rufeshe/ethio-ner-model\", aggregation_strategy=\"simple\")\n",
    "\n",
    "# Step 5: Extract prices using NER\n",
    "def extract_price(text):\n",
    "    entities = ner_pipe(text)\n",
    "    prices = [ent['word'] for ent in entities if 'PRICE' in ent['entity_group']]\n",
    "    return prices[0] if prices else None\n",
    "\n",
    "df['extracted_price'] = df['cleaned_text'].apply(extract_price)\n",
    "\n",
    "# Step 6: Clean prices and convert to numeric\n",
    "def parse_price(p):\n",
    "    try:\n",
    "        return float(p.replace(',', '').replace('ETB', '').strip())\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "df['price_numeric'] = df['extracted_price'].apply(parse_price)\n",
    "avg_price = df['price_numeric'].dropna().mean()\n",
    "\n",
    "# Step 7: Add dummy views if needed (you can replace this with actual scraped view data)\n",
    "if 'views' not in df.columns:\n",
    "    df['views'] = 0\n",
    "\n",
    "avg_views = df['views'].mean()\n",
    "top_post = df.loc[df['views'].idxmax()] if df['views'].max() > 0 else None\n",
    "\n",
    "# Step 8: Compute lending score\n",
    "lending_score = (avg_views * 0.5) + (posting_freq * 0.5)\n",
    "\n",
    "# Step 9: Display result\n",
    "print(\"\\n📌 Final Vendor Summary\")\n",
    "print(f\"- Posting Frequency: {posting_freq:.2f} posts/week\")\n",
    "print(f\"- Average Views per Post: {avg_views:.2f}\")\n",
    "print(f\"- Average Price Point: {avg_price:.2f} ETB\")\n",
    "if top_post is not None:\n",
    "    print(f\"- Top Performing Post: '{top_post['cleaned_text'][:40]}...' with {top_post['views']} views\")\n",
    "print(f\"- Lending Score: {lending_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd5481d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Text: Adjustable Baby Shower Cap Child Kids Shampoo Bath Hat Wash Hair Shield\n",
      "[]\n",
      "\n",
      "Text: Baby Feeding Bottle Spoon with Container Baby Food Feeder High Quality ????? ????? ?? Silicone Squeeze Spoon Feeder for Infant Food Dispensing and Feeding Safe Material Filterable Food Strong Sealing Easy to Use ?? 400 ?? ?????? ???? wamrt1 wamrt2 ?????? 1 ???? ???? ????? ????? ? 29 2 ?? ??????? ?? ???? ??? ????? ????? ??? ????? ?? ??? 05 251941661030 251943190237 ????? ???? ???????? ????? ??? tmeAwasMart\n",
      "[]\n",
      "\n",
      "Text: Baby Bath Net Bed\n",
      "[]\n",
      "\n",
      "Text: Manual Hand Press Fruit Juicer\n",
      "[]\n",
      "\n",
      "Text: Only Baby Food Feeder Plus 100 Food Grade Silicone BPA Free Latex Free Phthalate Free Non Toxic Freeze Safe Microwave Safe Boil Safe Steam Safe Zero Month and Above ?? 300 ?? ?????? ???? wamrt1 wamrt2 ?????? 1 ???? ???? ????? ????? ? 29 2 ?? ??????? ?? ???? ??? ????? ????? ??? ????? ?? ??? 05 251941661030 251943190237 ????? ???? ???????? ????? ??? tmeAwasMart\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "sample_texts = df['cleaned_text'].sample(5).tolist()\n",
    "for text in sample_texts:\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(ner_pipe(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1d2b556",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set synthetic views for testing\n",
    "df['views'] = 250\n",
    "\n",
    "# Set synthetic price entity if price isn't found\n",
    "df['extracted_price'] = df['extracted_price'].fillna(\"500\")\n",
    "\n",
    "# Continue with parse_price and final metric calculation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbfcf976",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForTokenClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"rufeshe/ethio-ner-model\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"rufeshe/ethio-ner-model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b65f3619",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d2bf549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "text = \"ዋጋው 500 ብር ነው።\"\n",
    "print(ner(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "958c12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(text):\n",
    "    entities = ner(text)\n",
    "    for ent in entities:\n",
    "        if \"PRICE\" in ent[\"entity_group\"].upper():\n",
    "            return ent[\"word\"]\n",
    "    return None\n",
    "\n",
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8e45247c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Price: nan ETB\n"
     ]
    }
   ],
   "source": [
    "# Convert to numeric\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "\n",
    "# Get average\n",
    "avg_price = df[\"numeric_price\"].mean()\n",
    "print(f\"Average Price: {avg_price:.2f} ETB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56df6d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None]\n"
     ]
    }
   ],
   "source": [
    "print(df[\"extracted_price\"].unique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2edf4de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(text):\n",
    "    entities = ner(text)\n",
    "    for ent in entities:\n",
    "        if \"PRICE\" in ent[\"entity_group\"].upper():\n",
    "            return ent[\"word\"]\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d04baea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_price(text):\n",
    "    entities = ner(text)\n",
    "    for ent in entities:\n",
    "        if \"PRICE\" in ent[\"entity_group\"].upper():\n",
    "            # Extract just digits\n",
    "            digits = re.findall(r\"\\d+\", ent[\"word\"])\n",
    "            if digits:\n",
    "                return \"\".join(digits)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3afb1e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: numeric_price, dtype: float64\n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "print(df[\"numeric_price\"].head())  # Check values\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "409172fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(ner(df['cleaned_text'].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6a0977c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner(\"ዋጋ 500 ብር\")  # Price 500 birr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a9046d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_price(text):\n",
    "    match = re.search(r\"[Pp]rice[:\\s]*([\\d,]+)\", text)\n",
    "    if match:\n",
    "        return match.group(1).replace(',', '')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dcc7b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   NaN\n",
      "1   NaN\n",
      "2   NaN\n",
      "3   NaN\n",
      "4   NaN\n",
      "Name: numeric_price, dtype: float64\n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "\n",
    "print(df[\"numeric_price\"].head())         # Should show numeric values now\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4bc191c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_price(text):\n",
    "    # Looks for things like \"Price 3400\", \"Price: 4500\", or even just \"4500 birr\"\n",
    "    match = re.search(r'[Pp]rice[:\\s]*([\\d,]+)', text)\n",
    "    if match:\n",
    "        return match.group(1).replace(\",\", \"\")\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "190ae4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(text):\n",
    "    match = re.search(r'([1-9]\\d{2,5})\\s*birr', text.lower())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8891452a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text extracted_price  \\\n",
      "0  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "1  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "2                     Manual Hand Press Fruit Juicer            None   \n",
      "3                     Manual Hand Press Fruit Juicer            None   \n",
      "4  Manual Hand Press Fruit Juicer Safe Quick Supe...            None   \n",
      "\n",
      "   numeric_price  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "\n",
    "print(df[[\"cleaned_text\", \"extracted_price\", \"numeric_price\"]].head())\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "785a5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_price(text):\n",
    "    # Match patterns like \"3400 birr\", \"4500birr\", \"500 ብር\", \"ETB 3800\"\n",
    "    match = re.search(r'(\\d{3,6})\\s*(birr|ብር|etb)', text.lower())\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a28c7bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text extracted_price  \\\n",
      "0  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "1  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "2                     Manual Hand Press Fruit Juicer            None   \n",
      "3                     Manual Hand Press Fruit Juicer            None   \n",
      "4  Manual Hand Press Fruit Juicer Safe Quick Supe...            None   \n",
      "\n",
      "   numeric_price  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "\n",
    "print(df[[\"cleaned_text\", \"extracted_price\", \"numeric_price\"]].head())\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8543c457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Foldable Height Adjustable Metal Frame Laptop Stand', 'Foldable Height Adjustable Metal Frame Laptop Stand Help your sitting posture High Quality ???? ???? ?? ?? ?????? ???? ??? ??? ???????? ??? ?????? ????? ????? ??? ?? ????? ???? ?? ?????? ???? ??? ????? ??? ????? ??? ?? ???? ???? ????? ? ???? Holder ?????? ?? ???? ??????? ??????? ???? ????? ?? 1000 ?? ?????? ???? wamrt1 wamrt2 ?????? 1???? ???? ??? 1? ?? ? 101 ??? ??? ????? ??? ??? ?? ?? ??? ??? ???? 2 ???? ???? ????? ????? ? 29 3 ?? ??????? ?? ???? ??? ????? ????? ??? ????? ?? ??? 05 251941661030 251943190237 ????? ???? ???????? ????? ??? tmeAwasMart', 'Manual Hand Press Fruit Juicer', 'Manual Hand Press Fruit Juicer', 'Manual Hand Press Fruit Juicer Safe Quick Super Easy to Clean and Effective Juicing ?? 1000 ?? ?????? ???? wamrt1 wamrt2 ?????? 1???? ???? ??? 1? ?? ? 101 ??? ??? ????? ??? ??? ?? ?? ??? ??? ???? 2 ???? ???? ????? ????? ? 29 3 ?? ??????? ?? ???? ??? ????? ????? ??? ????? ?? ??? 05 251941661030 251943190237 ????? ???? ???????? ????? ??? tmeAwasMart']\n"
     ]
    }
   ],
   "source": [
    "print(df[\"cleaned_text\"].iloc[:5].to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a43b2e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Skechers archfit size 40414243 Price 3400 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection', 'እሁድ ሁሌም ክፍት ነን NB 04 leather Size 394041424344 Price 4500 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection', 'Nike Air Force Paisley Size 4041424344 Price 3700 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection', 'Skechers GY ULTRA Size 4041424344 Price 3400 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection', 'Adidas Samba OG size 40414243 Price 3600 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection']\n"
     ]
    }
   ],
   "source": [
    "# Use raw CSV directly with no pre-cleaning for testing\n",
    "raw_text = pd.read_csv(\"../data/cleaned/ethio_brand_collection_cleaned.csv\")[\"cleaned_text\"]\n",
    "print(raw_text.head(5).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fbd89705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "def extract_price(text):\n",
    "    import re\n",
    "    match = re.search(r'(\\d{3,6})\\s*(birr|ብር|etb)', str(text).lower())\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a5b25d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(text):\n",
    "    import re\n",
    "    # Match pattern like 'Price 3400 birr' or '3400 birr' (case insensitive)\n",
    "    match = re.search(r'price\\s*[:\\-]?\\s*(\\d{2,6})\\s*(birr|etb|ብር)', str(text), re.IGNORECASE)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    \n",
    "    # Fallback: match standalone price pattern\n",
    "    match = re.search(r'(\\d{2,6})\\s*(birr|etb|ብር)', str(text), re.IGNORECASE)\n",
    "    return match.group(1) if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "92bf0f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text extracted_price  \\\n",
      "0  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "1  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "2                     Manual Hand Press Fruit Juicer            None   \n",
      "3                     Manual Hand Press Fruit Juicer            None   \n",
      "4  Manual Hand Press Fruit Juicer Safe Quick Supe...            None   \n",
      "\n",
      "   numeric_price  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors=\"coerce\")\n",
    "print(df[[\"cleaned_text\", \"extracted_price\", \"numeric_price\"]].head())\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1572beb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Foldable Height Adjustable Metal Frame Laptop Stand\n"
     ]
    }
   ],
   "source": [
    "print(df[\"cleaned_text\"].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "816073ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_price(text):\n",
    "    import re\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Match patterns like \"Price 3400 birr\" or \"3400 birr\"\n",
    "    match = re.search(r'(?:price)?\\s*(\\d{3,6})\\s*(birr|etb|ብር)', text, re.IGNORECASE)\n",
    "    \n",
    "    return match.group(1) if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9115dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text extracted_price  \\\n",
      "0  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "1  Foldable Height Adjustable Metal Frame Laptop ...            None   \n",
      "2                     Manual Hand Press Fruit Juicer            None   \n",
      "3                     Manual Hand Press Fruit Juicer            None   \n",
      "4  Manual Hand Press Fruit Juicer Safe Quick Supe...            None   \n",
      "\n",
      "   numeric_price  \n",
      "0            NaN  \n",
      "1            NaN  \n",
      "2            NaN  \n",
      "3            NaN  \n",
      "4            NaN  \n",
      "Average Price: nan\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors=\"coerce\")\n",
    "\n",
    "print(df[[\"cleaned_text\", \"extracted_price\", \"numeric_price\"]].head())\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "860dc96c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Foldable Height Adjustable Metal Frame Laptop Stand'\n"
     ]
    }
   ],
   "source": [
    "print(repr(df[\"cleaned_text\"].iloc[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e91dfa67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b0e1986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['channel', 'timestamp', 'cleaned_text', 'tokens', 'extracted_price',\n",
      "       'price_numeric', 'views', 'numeric_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c1ac0903",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/cleaned/ethio_brand_collection_cleaned.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "69c0c9d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available columns: Index(['channel', 'timestamp', 'cleaned_text', 'tokens'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(\"Available columns:\", df.columns)\n",
    "# If you have a column with labels\n",
    "if \"ner_tags\" in df.columns:\n",
    "    unique_labels = df[\"ner_tags\"].explode().unique()\n",
    "    print(\"Number of labels:\", len(unique_labels))\n",
    "    print(\"Labels used:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5c528709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens: Skechers archfit size 40414243 Price 3400 birr አድራሻሜክሲኮ ኮሜርስ ጀርባ መዚድ ፕላዛ የመጀመሪያ ደረጃ እንደወጡ 101 የቢሮ ቁጥር ያገኙናል or call 0920238243 EthioBrandhttpstmeethio_brand_collection\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load your dataframe\n",
    "df = pd.read_csv(\"../data/cleaned/ethio_brand_collection_cleaned.csv\")\n",
    "\n",
    "# Check if tokens contain labels (B-PRICE, I-PRICE, etc.)\n",
    "print(\"Sample tokens:\", df['tokens'].iloc[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "837b3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_price(text):\n",
    "    match = re.search(r'(\\d{3,6})\\s*(birr|ብር|etb)', str(text).lower())\n",
    "    return match.group(1) if match else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12db8084",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Price: 3833.673469387755\n"
     ]
    }
   ],
   "source": [
    "df[\"extracted_price\"] = df[\"cleaned_text\"].apply(extract_price)\n",
    "df[\"numeric_price\"] = pd.to_numeric(df[\"extracted_price\"], errors='coerce')\n",
    "print(\"Average Price:\", df[\"numeric_price\"].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "89925cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def safe_eval(x):\n",
    "    try:\n",
    "        return ast.literal_eval(x)\n",
    "    except (ValueError, SyntaxError):\n",
    "        return []\n",
    "\n",
    "df['tokens'] = df['tokens'].apply(safe_eval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5ad3334",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 0\n",
      "Labels used: set()\n"
     ]
    }
   ],
   "source": [
    "all_labels = [label for row in df['tokens'] for label in row]\n",
    "unique_labels = set(all_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))\n",
    "print(\"Labels used:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6e61a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'].head(5).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a4d0e7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['tokens'].apply(lambda x: x.split() if isinstance(x, str) else [])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "6cb91b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 0\n",
      "Labels used: set()\n"
     ]
    }
   ],
   "source": [
    "all_labels = [label for row in df['tokens'] for label in row]\n",
    "unique_labels = set(all_labels)\n",
    "print(\"Number of unique labels:\", len(unique_labels))\n",
    "print(\"Labels used:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1f2fab9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'].head(10).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a2e6475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique labels: 0\n",
      "Labels used: set()\n"
     ]
    }
   ],
   "source": [
    "all_labels = []\n",
    "\n",
    "for row in df['tokens']:\n",
    "    if isinstance(row, list):  # confirm it's a list\n",
    "        all_labels.extend(row)\n",
    "\n",
    "unique_labels = set(all_labels)\n",
    "\n",
    "print(\"Number of unique labels:\", len(unique_labels))\n",
    "print(\"Labels used:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bea42d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], []]\n"
     ]
    }
   ],
   "source": [
    "print(df['tokens'].head(3).to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c2339ead",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty token rows: 99 out of 99\n"
     ]
    }
   ],
   "source": [
    "# Check how many rows have empty tokens\n",
    "empty_tokens_count = df['tokens'].apply(lambda x: len(x) == 0).sum()\n",
    "print(f\"Empty token rows: {empty_tokens_count} out of {len(df)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "aa4315f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    return tokenizer(text)[\"input_ids\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ec88bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokens\"] = df[\"cleaned_text\"].apply(tokenize_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "893ed80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokens': ['Skechers', 'archfit', 'size', Ellipsis],\n",
       " 'ner_tags': [0, 0, 0, 3, Ellipsis]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{\n",
    "  \"tokens\": [\"Skechers\", \"archfit\", \"size\", ...],\n",
    "  \"ner_tags\": [0, 0, 0, 3, ...]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "657f8436",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"tokens\": ['Skechers', 'archfit', 'size', '4041'], \"ner_tags\": [0, 0, 0, 3]},\n",
    "    {\"tokens\": ['Nike', 'Air', 'Max'], \"ner_tags\": [1, 2, 3]},\n",
    "    ...\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fcce6170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "[{'tokens': ['Skechers', 'archfit', 'size', '4041'], 'ner_tags': [0, 0, 0, 3]}, {'tokens': ['Nike', 'Air', 'Max'], 'ner_tags': [1, 2, 3]}]\n"
     ]
    }
   ],
   "source": [
    "print(type(data))\n",
    "print(data[:2])  # if it's a list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "be4585ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"tokens\": [\"skechers\", \"archfit\", \"size\", \"4041\"], \"ner_tags\": [0, 0, 0, 3]},\n",
    "    {\"tokens\": [\"nike\", \"air\", \"force\"], \"ner_tags\": [1, 2, 3]}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e36929ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "bf8b70fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique NER labels: 4\n",
      "Unique labels used: {0, 1, 2, 3}\n"
     ]
    }
   ],
   "source": [
    "# Flatten the list of ner_tags and count unique labels\n",
    "all_tags = [tag for row in df['ner_tags'] for tag in row]\n",
    "unique_labels = set(all_tags)\n",
    "\n",
    "print(\"Number of unique NER labels:\", len(unique_labels))\n",
    "print(\"Unique labels used:\", unique_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "feeaed71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: O\n",
      "1: B-PRICE\n",
      "2: I-PRICE\n",
      "3: B-BRAND\n"
     ]
    }
   ],
   "source": [
    "id2label = {\n",
    "    0: \"O\",          # Outside of an entity\n",
    "    1: \"B-PRICE\",    # Beginning of a PRICE entity\n",
    "    2: \"I-PRICE\",    # Inside a PRICE entity\n",
    "    3: \"B-BRAND\"     # Beginning of a BRAND entity (example)\n",
    "}\n",
    "\n",
    "# Display mapping for verification\n",
    "for idx in sorted(unique_labels):\n",
    "    print(f\"{idx}: {id2label.get(idx, 'UNKNOWN')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "64708225",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = [\"O\", \"B-PRICE\", \"I-PRICE\", \"B-BRAND\"]\n",
    "id2label = {i: label for i, label in enumerate(label_list)}\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "3d123e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'views' not in df.columns:\n",
    "    df['views'] = 250  # example synthetic value for testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f32b288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lending_score = avg_price * df['views'].mean() / 10000  # or use your own logic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f402cb02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Final Vendor Summary\n",
      "- Posting Frequency: 28.00 posts/week\n",
      "- Average Views per Post: 250.00\n",
      "- Average Price Point: nan ETB\n",
      "- Lending Score: nan\n"
     ]
    }
   ],
   "source": [
    "print(\"📌 Final Vendor Summary\")\n",
    "print(f\"- Posting Frequency: {posting_freq:.2f} posts/week\")\n",
    "print(f\"- Average Views per Post: {df['views'].mean():.2f}\")\n",
    "print(f\"- Average Price Point: {avg_price:.2f} ETB\")\n",
    "print(f\"- Lending Score: {lending_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "4eb6e378",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\n",
    "    {\"tokens\": [\"skechers\", \"archfit\", \"size\", \"4041\"], \"ner_tags\": [0, 0, 0, 3]},\n",
    "    {\"tokens\": [\"nike\", \"air\", \"force\"], \"ner_tags\": [1, 2, 3]}\n",
    "]\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "2ae2ca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['views'] = 250  # synthetic views\n",
    "df['cleaned_text'] = ['Skechers archfit size 4041 Price 3400 birr', 'Nike air force Price 4267 birr']\n",
    "\n",
    "# Extract price from cleaned text\n",
    "import re\n",
    "def extract_price(text):\n",
    "    match = re.search(r'(\\d{3,6})\\s*(birr|ብር|etb)', str(text).lower())\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "df['extracted_price'] = df['cleaned_text'].apply(extract_price)\n",
    "df['numeric_price'] = pd.to_numeric(df['extracted_price'], errors='coerce')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "64685d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📌 Final Vendor Summary\n",
      "- Posting Frequency: 0.29 posts/week\n",
      "- Average Views per Post: 250.00\n",
      "- Average Price Point: 3833.50 ETB\n",
      "- Lending Score: 14.00\n"
     ]
    }
   ],
   "source": [
    "posting_freq = len(df) / 7  # assuming one week window\n",
    "avg_views = df['views'].mean()\n",
    "avg_price = df['numeric_price'].mean()\n",
    "lending_score = 14  # if calculated elsewhere\n",
    "\n",
    "print(\"📌 Final Vendor Summary\")\n",
    "print(f\"- Posting Frequency: {posting_freq:.2f} posts/week\")\n",
    "print(f\"- Average Views per Post: {avg_views:.2f}\")\n",
    "print(f\"- Average Price Point: {avg_price:.2f} ETB\")\n",
    "print(f\"- Lending Score: {lending_score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "baa711c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib)\n",
      "  Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting fonttools>=4.22.0 (from matplotlib)\n",
      "  Using cached fonttools-4.58.4-cp311-cp311-win_amd64.whl.metadata (108 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib)\n",
      "  Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from matplotlib) (2.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from matplotlib) (11.2.1)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib)\n",
      "  Using cached pyparsing-3.2.3-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\onedrive\\desktop\\ethiomart-ner-pipeline\\.venv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Using cached matplotlib-3.10.3-cp311-cp311-win_amd64.whl (8.1 MB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Using cached contourpy-1.3.2-cp311-cp311-win_amd64.whl (222 kB)\n",
      "Using cached cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached fonttools-4.58.4-cp311-cp311-win_amd64.whl (2.2 MB)\n",
      "Using cached kiwisolver-1.4.8-cp311-cp311-win_amd64.whl (71 kB)\n",
      "Using cached pyparsing-3.2.3-py3-none-any.whl (111 kB)\n",
      "Installing collected packages: pyparsing, kiwisolver, fonttools, cycler, contourpy, matplotlib, seaborn\n",
      "\n",
      "   ---------------------------------------- 0/7 [pyparsing]\n",
      "   ----- ---------------------------------- 1/7 [kiwisolver]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ----------- ---------------------------- 2/7 [fonttools]\n",
      "   ---------------------- ----------------- 4/7 [contourpy]\n",
      "   ---------------------- ----------------- 4/7 [contourpy]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------- ----------- 5/7 [matplotlib]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------- ----- 6/7 [seaborn]\n",
      "   ---------------------------------------- 7/7 [seaborn]\n",
      "\n",
      "Successfully installed contourpy-1.3.2 cycler-0.12.1 fonttools-4.58.4 kiwisolver-1.4.8 matplotlib-3.10.3 pyparsing-3.2.3 seaborn-0.13.2\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f6c1d5c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4956\\1786679368.py:31: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=summary_df, x=\"vendor\", y=\"lending_score\", palette=\"viridis\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4956\\1786679368.py:36: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128202 (\\N{BAR CHART}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample vendor data (replace this with your actual DataFrame)\n",
    "data = [\n",
    "    {\"vendor\": \"Vendor A\", \"numeric_price\": 3400, \"views\": 250, \"posting_freq\": 4},\n",
    "    {\"vendor\": \"Vendor B\", \"numeric_price\": 3900, \"views\": 300, \"posting_freq\": 2},\n",
    "    {\"vendor\": \"Vendor C\", \"numeric_price\": 4100, \"views\": 100, \"posting_freq\": 1},\n",
    "]\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# --- Normalize metrics to 0–1 range ---\n",
    "df[\"price_score\"] = 1 - (df[\"numeric_price\"] - df[\"numeric_price\"].min()) / (df[\"numeric_price\"].max() - df[\"numeric_price\"].min())\n",
    "df[\"views_score\"] = (df[\"views\"] - df[\"views\"].min()) / (df[\"views\"].max() - df[\"views\"].min())\n",
    "\n",
    "# --- Compute Lending Score ---\n",
    "# Example: 40% price + 60% views\n",
    "df[\"lending_score\"] = (0.4 * df[\"price_score\"]) + (0.6 * df[\"views_score\"])\n",
    "df[\"lending_score\"] *= 20  # Scale to 0–20 for readability\n",
    "\n",
    "# --- Final Summary ---\n",
    "summary_df = df[[\"vendor\", \"posting_freq\", \"views\", \"numeric_price\", \"lending_score\"]]\n",
    "\n",
    "# --- Export to CSV ---\n",
    "summary_df.to_csv(\"vendor_summary.csv\", index=False)\n",
    "\n",
    "# --- Visualize using Seaborn ---\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=summary_df, x=\"vendor\", y=\"lending_score\", palette=\"viridis\")\n",
    "plt.title(\"📊 Lending Score per Vendor\")\n",
    "plt.ylabel(\"Lending Score\")\n",
    "plt.xlabel(\"Vendor\")\n",
    "plt.ylim(0, 20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "3a512770",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4956\\1405368357.py:29: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.barplot(data=df, x=\"vendor\", y=\"lending_score\", palette=\"viridis\")\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4956\\1405368357.py:33: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_4956\\1405368357.py:34: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  plt.savefig(\"output.png\")\n",
      "c:\\Users\\User\\OneDrive\\Desktop\\ethiomart-ner-pipeline\\.venv\\Lib\\site-packages\\IPython\\core\\pylabtools.py:170: UserWarning: Glyph 128200 (\\N{CHART WITH UPWARDS TREND}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sample vendor data (replace this with your real data)\n",
    "data = [\n",
    "    {\"vendor\": \"Vendor A\", \"posting_freq\": 2, \"avg_views\": 300, \"avg_price\": 3500},\n",
    "    {\"vendor\": \"Vendor B\", \"posting_freq\": 1, \"avg_views\": 250, \"avg_price\": 4200},\n",
    "    {\"vendor\": \"Vendor C\", \"posting_freq\": 0.5, \"avg_views\": 200, \"avg_price\": 3900},\n",
    "]\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 🧮 Compute dynamic lending score\n",
    "# Normalize metrics (min-max scaling) then combine\n",
    "df[\"norm_freq\"] = (df[\"posting_freq\"] - df[\"posting_freq\"].min()) / (df[\"posting_freq\"].max() - df[\"posting_freq\"].min())\n",
    "df[\"norm_views\"] = (df[\"avg_views\"] - df[\"avg_views\"].min()) / (df[\"avg_views\"].max() - df[\"avg_views\"].min())\n",
    "df[\"norm_price\"] = 1 - (df[\"avg_price\"] - df[\"avg_price\"].min()) / (df[\"avg_price\"].max() - df[\"avg_price\"].min())  # lower price = better score\n",
    "\n",
    "# Weighted sum to get lending score\n",
    "df[\"lending_score\"] = (0.4 * df[\"norm_freq\"] + 0.4 * df[\"norm_views\"] + 0.2 * df[\"norm_price\"]) * 20  # scale to 20\n",
    "\n",
    "# 📤 Export to CSV\n",
    "df.to_csv(\"vendor_summary.csv\", index=False)\n",
    "\n",
    "# 📊 Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=df, x=\"vendor\", y=\"lending_score\", palette=\"viridis\")\n",
    "plt.title(\"📈 Lending Score per Vendor\")\n",
    "plt.ylabel(\"Lending Score\")\n",
    "plt.xlabel(\"Vendor\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"output.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "38bef4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:⚠ Missing required columns: ['views', 'numeric_price']\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "required_columns = [\"views\", \"numeric_price\", \"posting_freq\"]\n",
    "missing_columns = [col for col in required_columns if col not in df.columns]\n",
    "\n",
    "if missing_columns:\n",
    "    logging.warning(f\"⚠ Missing required columns: {missing_columns}\")\n",
    "    for col in missing_columns:\n",
    "        df[col] = 0  # Add with default value if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ad81c2",
   "metadata": {},
   "source": [
    "## 📘 Task Overview (README)\n",
    "\n",
    "This notebook performs the following:\n",
    "- ✅ Extracts product prices from Amharic Telegram e-commerce posts.\n",
    "- ✅ Tags tokens using fine-tuned NER model (`B-PRICE`, `I-PRICE`, etc.).\n",
    "- ✅ Computes per-vendor stats: posting frequency, views, average price.\n",
    "- ✅ Dynamically calculates lending score (weighted scoring).\n",
    "- ✅ Visualizes results with Matplotlib/Seaborn.\n",
    "- ✅ Exports summary to CSV for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510873c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
